<!DOCTYPE html><html><head><meta charset="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><link rel="shortcut icon" type="image/png" href="data:image/png;base64,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"/><link rel="preload" href="/myblog/component---src-layouts-index-js-c1e083e7d85dd0aca005.js" as="script"/><link rel="preload" href="/myblog/component---src-templates-blog-post-js-55b2a1a99d1ea213b953.js" as="script"/><link rel="preload" href="/myblog/path---posts-10-660bb478f0dd0283a544.js" as="script"/><link rel="preload" href="/myblog/app-df37403cdd0e6ac7ebee.js" as="script"/><link rel="preload" href="/myblog/commons-68aad81503ae45628ade.js" as="script"/><script id="webpack-manifest">/*<![CDATA[*/window.webpackManifest={"231608221292675":"app-df37403cdd0e6ac7ebee.js","107818501498521":"component---src-templates-blog-post-js-55b2a1a99d1ea213b953.js","263791100135453":"component---src-pages-about-js-ff45d29fd778b5a39075.js","35783957827783":"component---src-pages-index-js-31d5d9c1c3bd56028ef0.js","60335399758886":"path----557518bd178906f8d58a.js","81878219163735":"path---posts-3-65ce2f5b9207722d610a.js","216243879586913":"path---posts-2-f92f30ac7d2d8cb243fe.js","91246642375014":"path---posts-1-90166e225d03eda6390c.js","123118983598882":"path---posts-4-0542fcb9ca1a645193f6.js","4876863402267":"path---posts-5-1c348bc4f0bbbffb828e.js","106140644916941":"path---posts-6-654c88b5731d540ed597.js","130752015456929":"path---posts-7-f6ed77d92a2e941ca489.js","93923592088315":"path---posts-8-7cce6f2a475c28aa3699.js","20147626466865":"path---posts-9-b20d26b952031219dcce.js","142656853184902":"path---posts-10-660bb478f0dd0283a544.js","244307437753867":"path---posts-11-64142ec9824c2a7d7728.js","279941687692024":"path---posts-12-a0324a2f6994102c920b.js","273950069227526":"path---about-a0e39f21c11f6a62c5ab.js","142629428675168":"path---index-61b1d83abf6f9d216171.js","114276838955818":"component---src-layouts-index-js-c1e083e7d85dd0aca005.js"}/*]]>*/</script><style type="text/css" data-styled-components="kNnBpQ iKSQLe djlcST ZZrCW cDUvTM iLZaUK jorVQO" data-styled-components-is-local="true">/* sc-component-id: sc-bdVaJa */

.kNnBpQ{height:100vh;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}
/* sc-component-id: sc-bwzfXH */

.djlcST{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;position:relative;}
/* sc-component-id: sc-htpNat */

.ZZrCW{position:absolute;top:0;left:0;right:0;bottom:0;}
/* sc-component-id: sc-bxivhb */

.iKSQLe{line-height:40px;padding:10px;background:#00bcd4;}.iKSQLe a{color:white;}
/* sc-component-id: sc-ifAKCX */

.jorVQO{background:#00bcd4;padding:28px;color:white;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}.jorVQO a{color:white;display:block;padding:0px;}
/* sc-component-id: sc-EHOje */

.cDUvTM{height:100%;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding-top:20px;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.cDUvTM .markdown-content{padding:20px;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}
/* sc-component-id: sc-bZQynM */

.iLZaUK{font-size:20px;text-align:center;font-weight:600px;}
</style><script>/*<![CDATA[*/!function(e,t,r){function n(){for(;d[0]&&"loaded"==d[0][f];)c=d.shift(),c[o]=!i.parentNode.insertBefore(c,i)}for(var s,a,c,d=[],i=e.scripts[0],o="onreadystatechange",f="readyState";s=r.shift();)a=e.createElement(t),"async"in i?(a.async=!1,e.head.appendChild(a)):i[f]?(d.push(a),a[o]=n):e.write("<"+t+' src="'+s+'" defer></'+t+">"),a.src=s}(document,"script",["/myblog/commons-68aad81503ae45628ade.js","/myblog/app-df37403cdd0e6ac7ebee.js","/myblog/path---posts-10-660bb478f0dd0283a544.js","/myblog/component---src-templates-blog-post-js-55b2a1a99d1ea213b953.js","/myblog/component---src-layouts-index-js-c1e083e7d85dd0aca005.js"])/*]]>*/</script><style id="gatsby-inlined-css">code[class*=language-],pre[class*=language-]{color:#657b83;font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-ms-hyphens:none;hyphens:none}code[class*=language-]::-moz-selection,code[class*=language-] ::-moz-selection,pre[class*=language-]::-moz-selection,pre[class*=language-] ::-moz-selection{background:#073642}code[class*=language-]::selection,code[class*=language-] ::selection,pre[class*=language-]::selection,pre[class*=language-] ::selection{background:#073642}pre[class*=language-]{padding:1em;margin:.5em 0;overflow:auto;border-radius:.3em}:not(pre)>code[class*=language-],pre[class*=language-]{background-color:#fdf6e3}:not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em}.token.cdata,.token.comment,.token.doctype,.token.prolog{color:#93a1a1}.token.punctuation{color:#586e75}.namespace{opacity:.7}.token.boolean,.token.constant,.token.deleted,.token.number,.token.property,.token.symbol,.token.tag{color:#268bd2}.token.attr-name,.token.builtin,.token.char,.token.inserted,.token.selector,.token.string,.token.url{color:#2aa198}.token.entity{color:#657b83;background:#eee8d5}.token.atrule,.token.attr-value,.token.keyword{color:#859900}.token.function{color:#b58900}.token.important,.token.regex,.token.variable{color:#cb4b16}.token.bold,.token.important{font-weight:700}.token.italic{font-style:italic}.token.entity{cursor:help}body{margin:0}a{text-decoration:none}*{box-sizing:border-box}</style></head><body><div id="___gatsby"><div class="sc-bdVaJa kNnBpQ" data-reactroot="" data-reactid="1" data-react-checksum="651435522"><div class="sc-bxivhb iKSQLe" data-reactid="2"><a href="/myblog/" data-reactid="3">首页</a></div><div class="sc-bwzfXH djlcST" data-reactid="4"><div class="sc-htpNat ZZrCW" data-reactid="5"><div class="sc-EHOje cDUvTM" data-reactid="6"><!-- react-empty: 7 --><div class="sc-bZQynM iLZaUK" data-reactid="8">Keras 快速上手指南（中）</div><div class="markdown-content" data-reactid="9"><p>1. Keras 模型
Keras有两种类型的模型，序贯模型(Sequential)和函数式模型(Model)，函数式模型应用更为广泛，序贯模型是函数式模型的一种特殊情况。</p>
<div class="gatsby-highlight">
      <pre class="language-python"><code>两类模型有一些方法是相同的：

model<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span>：打印出模型概况
model<span class="token punctuation">.</span>get_config<span class="token punctuation">(</span><span class="token punctuation">)</span>：返回包含模型配置信息的 Python 字典。模型也可以从它的 config 信息中重构回去
config <span class="token operator">=</span> model<span class="token punctuation">.</span>get_config<span class="token punctuation">(</span><span class="token punctuation">)</span>

model <span class="token operator">=</span> Model<span class="token punctuation">.</span>from_config<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
model <span class="token operator">=</span> Sequential<span class="token punctuation">.</span>from_config<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
model<span class="token punctuation">.</span>get_layer<span class="token punctuation">(</span><span class="token punctuation">)</span>：依据层名或下标获得层对象
model<span class="token punctuation">.</span>get_weights<span class="token punctuation">(</span><span class="token punctuation">)</span>：返回模型权重张量的列表，类型为 numpy array
model<span class="token punctuation">.</span>set_weights<span class="token punctuation">(</span><span class="token punctuation">)</span>：从 numpy array 里将权重载入给模型，要求数组具有与 model<span class="token punctuation">.</span>get_weights<span class="token punctuation">(</span><span class="token punctuation">)</span> 相同的形状。
model<span class="token punctuation">.</span>save_weights<span class="token punctuation">(</span>filepath<span class="token punctuation">)</span>：将模型权重保存到指定路径，文件类型是 HDF5（后缀是 <span class="token punctuation">.</span>h5）
model<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span>filepath<span class="token punctuation">,</span> by_name<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>：从 HDF5 文件中加载权重到当前模型中<span class="token punctuation">,</span> 默认情况下模型的结构将保持不变。如果想将权重载入不同的模型（有些层相同）中，则设置 by_name<span class="token operator">=</span><span class="token boolean">True</span>，只有名字匹配的层才会载入权重
</code></pre>
      </div>
<ol start="2">
<li>
<p>Sequential 模型接口
在Sequntial 模型中，我们已经介绍了模型的基本使用方法和实例，下面我们进一步对 Sequential 的 API 和参数做详细的介绍。</p>
<div class="gatsby-highlight">
      <pre class="language-python"><code>首先我们了解一下常用的 Sequential 属性：
</code></pre>
      </div>
</li>
</ol>
<p>model.layers 是添加到模型上的层的 list
注：考虑到本文只是快速指南，因而省略了部分高级的属性和方法。</p>
<p>add 添加层
向模型中添加一个层。</p>
<p>add(self, layer)
layer: Layer 对象
pop 删除层
弹出模型最后的一层，无返回值</p>
<p>pop(self)
compile 编译模型
compile(self, optimizer, loss, metrics=None)
optimizer：字符串（预定义优化器名）或优化器对象
loss：字符串（预定义损失函数名）或目标函数
metrics：列表，包含评估模型在训练和测试时的网络性能的指标，典型用法是 metrics=['accuracy']
model = Sequential()
model.add(Dense(32, input<em>shape=(500,)))
model.add(Dense(10, activation='softmax'))
model.compile(optimizer='rmsprop',
loss='categorical</em>crossentropy',
metrics=['accuracy'])
模型在使用前必须编译，否则在调用 fit 或 evaluate 时会抛出异常。</p>
<p>如果你只是载入模型并利用其 predict，可以不用进行 compile。在 Keras 中，compile 主要完成损失函数和优化器的一些配置，是为训练服务的。predict 会在内部进行符号函数的编译工作。</p>
<p>fit 训练模型
将模型训练 nb_epoch 轮。</p>
<p>fit(self, x, y, batch<em>size=32, epochs=10, verbose=1, validation</em>split=0.0, validation<em>data=None, shuffle=True)
x：输入数据。如果模型只有一个输入，那么 x 的类型是 numpy array，如果模型有多个输入，那么 x 的类型应当为 list，list 的元素是对应于各个输入的 numpy array
y：标签，numpy array
batch</em>size：整数，指定进行梯度下降时每个 batch 包含的样本数。训练时一个 batch 的样本会被计算一次梯度下降，使目标函数优化一步。
epochs：整数，训练的轮数，每个 epoch 会把训练集轮一遍。
verbose：日志显示，0 为不在标准输出流输出日志信息，1 为输出进度条记录，2 为每个epoch输出一行记录
validation<em>split：0~1 之间的浮点数，用来指定训练集的一定比例数据作为验证集。验证集将不参与训练，并在每个 epoch 结束后测试的模型的指标，如损失函数、精确度等。注意，validation</em>split 的划分在 shuffle 之前，因此如果你的数据本身是有序的，需要先手工打乱再指定 validation<em>split，否则可能会出现验证集样本不均匀。
validation</em>data：形式为 (X，y) 的 tuple，是指定的验证集。此参数将覆盖 validation_spilt。
shuffle：布尔值或字符串，一般为布尔值，表示是否在训练过程中随机打乱输入样本的顺序。若为字符串“batch”，则是用来处理HDF5数据的特殊情况，它将在batch内部将数据打乱。
fit 函数返回一个 History 的对象，其 History.history 属性记录了损失函数和其他指标的数值随 epoch 变化的情况，如果有验证集的话，也包含了验证集的这些指标变化情况。</p>
<p>evaluate 测试模型
按 batch 计算在某些输入数据上模型的误差</p>
<p>evaluate(self, x, y, batch<em>size=32, verbose=1)
x：输入数据，与 fit 一样，是 numpy array 或 numpy array 的 list
y：标签，numpy array
batch</em>size：整数，指定进行测试时每个 batch 包含的样本数。
verbose：日志显示，0 为不在标准输出流输出日志信息，1 为输出进度条记录
本函数返回一个测试误差的标量值（如果模型没有其他评价指标），或一个标量的 list（如果模型还有其他的评价指标）。model.metrics_names 将给出 list 中各个值的含义。</p>
<p>predict 预测
按 batch 获得输入数据对应的预测结果，返回各个类别的可能性结果，是一个 n 维向量，n 等于类别的数量。</p>
<p>predict(self, x, batch<em>size=32, verbose=0)
x：输入数据，与 fit 一样，是 numpy array 或 numpy array 的 list
batch</em>size：整数，每批次选取的数据集数量
verbose：日志显示，0 为不在标准输出流输出日志信息，1 为输出进度条记录
predict_classes 预测类别
按 batch 产生输入数据的类别预测结果，返回的是最可能的类别名称。</p>
<p>predict<em>classes(self, x, batch</em>size=32, verbose=1)</p>
<ol start="3">
<li>函数式模型接口
Keras 的函数式模型为 Model，即广义的拥有输入和输出的模型，我们使用 Model 来初始化一个函数式模型：</li>
</ol>
<p>from keras.models import Model
from keras.layers import Input, Dense</p>
<p>a = Input(shape=(32,))
b = Dense(32)(a)
model = Model(inputs=a, outputs=b)
在这里，我们的模型以 a 为输入，以 b 为输出，同样我们可以构造拥有多输入和多输出的模型：</p>
<p>model = Model(inputs=[a1, a2], outputs=[b1, b3, b3])
在Functional 模型中，我们已经介绍了模型的基本使用方法和实例，下面我们进一步对函数式模型的 API 和参数做详细的介绍。</p>
<p>首先我们了解一下常用的 Modell 属性：</p>
<p>model.layers：组成模型图的各个层
model.inputs：模型的输入张量列表
model.outputs：模型的输出张量列表
Model 模型的方法除了没有 add、pop 方法外与 Sequential 模型几乎是相同的，这里就不做赘述了。</p>
<div class="gatsby-highlight">
      <pre class="language-none"><code>4. Keras 层
所有的 Keras 层对象都有如下方法：
```python
layer.get_weights()：返回层的权重（numpy array）
layer.set_weights(weights)：从 numpy array 中将权重加载到该层中，要求 numpy array 的形状与 layer.get_weights() 的形状相同
layer.get_config()：返回当前层配置信息的字典，层也可以借由配置信息重构:
layer = Dense(32)
config = layer.get_config()
reconstructed_layer = Dense.from_config(config)
或者：

from keras import layers

config = layer.get_config()
layer = layers.deserialize({'class_name': layer.__class__.__name__,
                            'config': config})
如果层仅有一个计算节点（即该层不是共享层），则可以通过下列方法获得输入张量、输出张量、输入数据的形状和输出数据的形状：

layer.input
layer.output
layer.input_shape
layer.output_shape
如果该层有多个计算节点（参考层计算节点和共享层）。可以使用下面的方法：

layer.get_input_at(node_index)
layer.get_output_at(node_index)
layer.get_input_shape_at(node_index)
layer.get_output_shape_at(node_index)</code></pre>
      </div>
<ol start="5">
<li>
<p>常用层
常用层对应于 core 模块，core 内部定义了一系列常用的网络层，包括全连接、激活层等。</p>
<div class="gatsby-highlight">
      <pre class="language-python"><code>Dense 层
Dense 就是全连接层。
</code></pre>
      </div>
</li>
</ol>
<p>keras.layers.core.Dense(units, activation=None)
units：大于 0 的整数，代表该层的输出维度。
activation：激活函数，为预定义的激活函数名。如果不指定该参数，将不会使用任何激活函数（即使用线性激活函数：
a
(
x
)
=
x
a(x)=x）
该层的输入为形如 (nb<em>samples, …, input</em>shape[1]) 的 nD 张量，最常见的情况为 (nb<em>samples, input</em>dim) 的 2D 张量。输出为形如 (nb<em>samples, …, units) 的 nD 张量，最常见的情况为 (nb</em>samples, output_dim) 的 2D 张量。</p>
<h1>作为sequential模型的第一层输入:</h1>
<p>model = Sequential()
model.add(Dense(32, input_shape=(16,)))</p>
<h1>模型输入shape为 (<em>, 16)，输出shape (</em>, 32)</h1>
<h1>第一层之后，就不需要指定输入大小了</h1>
<p>model.add(Dense(32))
Activation 层
激活层对一个层的输出施加激活函数。</p>
<p>keras.layers.core.Activation(activation)
activation：将要使用的激活函数，为预定义激活函数名或一个 Tensorflow/Theano 的函数。
该层输入 shape 任意，当使用激活层作为第一层时，要指定 input_shape。输出 shape 与输入 shape 相同。</p>
<p>Dropout 层
Dropout 层将在训练过程中每次更新参数时随机断开一定百分比（rate）的输入神经元，用于防止过拟合。</p>
<p>keras.layers.core.Dropout(rate, seed=None)
rate：0~1 的浮点数，控制需要断开的神经元的比例
seed：整数，使用的随机数种子
Flatten 层
Flatten 层用来将输入“压平”，即把多维的输入一维化，常用在从卷积层到全连接层的过渡。Flatten 不影响 batch 的大小。</p>
<p>keras.layers.core.Flatten()
model = Sequential()
model.add(Convolution2D(64, 3, 3,
border<em>mode='same',
input</em>shape=(3, 32, 32)))</p>
<h1>model.output_shape == (None, 64, 32, 32)</h1>
<p>model.add(Flatten())</p>
<h1>model.output_shape == (None, 65536)</h1>
<p>卷积操作将 (3,32,32) 格式的图像数据转换成 (64,32,32) 格式，Flatten 的结果则是 64 × 32 × 32 = 65536。</p>
<p>Reshape 层
Reshape 层用来将输入 shape 转换为特定的 shape。</p>
<p>keras.layers.core.Reshape(target<em>shape)
target</em>shape：目标 shape，为整数的 tuple，不包含样本数目的维度（batch 大小）
该层的输入 shape 任意，但必须固定。当使用该层为模型首层时，需要指定 input<em>shape 参数。输出 shape 为 (batch</em>size,)+target_shape。</p>
<h1>作为Sequential模型的第一层</h1>
<p>model = Sequential()
model.add(Reshape((3, 4), input_shape=(12,)))</p>
<h1>model.output_shape == (None, 3, 4)</h1>
<h1>注: <code>None</code> 是样本数目维</h1>
<h1>作为Sequential模型的中间层</h1>
<p>model.add(Reshape((6, 2)))</p>
<h1>model.output_shape == (None, 6, 2)</h1>
<h1>也支持 <code>-1</code> 作为维度</h1>
<p>model.add(Reshape((-1, 2, 2)))</p>
<h1>model.output_shape == (None, 3, 2, 2)</h1>
<div class="gatsby-highlight">
      <pre class="language-none"><code>6. 卷积层
Conv1D 层
一维卷积层（即时域卷积），用以在一维输入信号上进行邻域滤波。当使用该层作为首层时，需要提供关键字参数 input_shape。例如 (10,128) 代表一个长为 10 的序列，序列中每个信号为 128 向量。而 (None, 128) 代表变长的 128 维向量序列。

该层生成将输入信号与卷积核按照单一的空域（或时域）方向进行卷积。如果 use_bias=True，则还会加上一个偏置项，若 activation 不为 None，则输出为经过激活函数的输出。
```python
keras.layers.convolutional.Conv1D(filters, kernel_size, strides=1, padding='valid', dilation_rate=1, activation=None)
filters：卷积核的数目（即输出的维度）
kernel_size：整数或由单个整数构成的 list/tuple，卷积核的空域或时域窗长度
strides：整数或由单个整数构成的 list/tuple，为卷积的步长。任何不为 1 的 strides 均与任何不为 1 的 dilation_rata 均不兼容
padding：补 0 策略，为“valid”、“same” 或“causal”，“causal”将产生因果（膨胀的）卷积，即 output[t] 不依赖于 input[t+1：]，当对不能违反时间顺序的时序信号建模时有用。“valid”代表只进行有效的卷积，即对边界数据不处理。“same”代表保留边界处的卷积结果，通常会导致输出 shape 与输入 shape 相同。
activation：激活函数，为预定义的激活函数名。如果不指定该参数，将不会使用任何激活函数（即使用线性激活函数：
a
(
x
)
=
x
a(x)=x）
dilation_rate：整数或由单个整数构成的list/tuple，指定dilated convolution中的膨胀比例。任何不为1的dilation_rata均与任何不为1的strides均不兼容。
该层输入 shape 为形如 (samples，steps，input_dim) 的 3D 张量。输出 shape 为形如 (samples，new_steps，nb_filter) 的 3D 张量，因为有向量填充的原因，steps 的值会改变。

可以将 Convolution1D 看作 Convolution2D 的快捷版，对 (10，32) 的信号进行 1D 卷积相当于对其进行卷积核为 (filter_length, 32) 的 2D 卷积。</code></pre>
      </div>
<p>Conv2D 层
二维卷积层，即对图像的空域卷积。该层对二维输入进行滑动窗卷积，当使用该层作为第一层时，应提供 input<em>shape 参数。例如 input</em>shape = (128,128,3) 代表 128×128 的彩色 RGB 图像（data<em>format=’channels</em>last’）。</p>
<div class="gatsby-highlight">
      <pre class="language-python"><code>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>convolutional<span class="token punctuation">.</span>Conv2D<span class="token punctuation">(</span>filters<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'valid'</span><span class="token punctuation">,</span> data_format<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dilation_rate<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
filters：卷积核的数目（即输出的维度）
kernel_size：单个整数或由两个整数构成的 <span class="token builtin">list</span><span class="token operator">/</span><span class="token builtin">tuple</span>，卷积核的宽度和长度。如为单个整数，则表示在各个空间维度的相同长度。
strides：单个整数或由两个整数构成的 <span class="token builtin">list</span><span class="token operator">/</span><span class="token builtin">tuple</span>，为卷积的步长。如为单个整数，则表示在各个空间维度的相同步长。任何不为 <span class="token number">1</span> 的strides均与任何不为 <span class="token number">1</span> 的 dilation_rata 均不兼容
padding：补 <span class="token number">0</span> 策略，为“valid”<span class="token punctuation">,</span> “same” 。“valid”代表只进行有效的卷积，即对边界数据不处理。“same”代表保留边界处的卷积结果，通常会导致输出 shape 与输入 shape 相同。
activation：激活函数，为预定义的激活函数名。如果不指定该参数，将不会使用任何激活函数（即使用线性激活函数：
a
<span class="token punctuation">(</span>
x
<span class="token punctuation">)</span>
<span class="token operator">=</span>
x
a<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token operator">=</span>x）
dilation_rate：单个整数或由两个个整数构成的 <span class="token builtin">list</span><span class="token operator">/</span><span class="token builtin">tuple</span>，指定 dilated convolution 中的膨胀比例。任何不为 <span class="token number">1</span> 的 dilation_rata 均与任何不为 <span class="token number">1</span> 的 strides 均不兼容。
data_format：字符串，“channels_first”或“channels_last”之一，代表图像的通道维的位置。以 <span class="token number">128</span>×<span class="token number">128</span> 的 RGB 图像为例，“channels_first”应将数据组织为 <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">)</span>，而“channels_last”应将数据组织为 <span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>。该参数的默认值是 <span class="token operator">~</span><span class="token operator">/</span><span class="token punctuation">.</span>keras<span class="token operator">/</span>keras<span class="token punctuation">.</span>json 中设置的值，若从未设置过，则为“channels_last”。
该层输入 shape 在“channels_first”模式下，输入形如 <span class="token punctuation">(</span>samples<span class="token punctuation">,</span>channels，rows，cols<span class="token punctuation">)</span> 的 <span class="token number">4D</span> 张量，在“channels_last”模式下，输入形如 <span class="token punctuation">(</span>samples，rows，cols，channels<span class="token punctuation">)</span> 的 <span class="token number">4D</span> 张量。

注意：这里的输入 shape 指的是函数内部实现的输入 shape，而非函数接口应指定的 input_shape。

输出 shape 在“channels_first”模式下，为形如 <span class="token punctuation">(</span>samples，nb_filter<span class="token punctuation">,</span> new_rows<span class="token punctuation">,</span> new_cols<span class="token punctuation">)</span> 的 <span class="token number">4D</span> 张量，在“channels_last”模式下，为形如 <span class="token punctuation">(</span>samples，new_rows<span class="token punctuation">,</span> new_cols，nb_filter<span class="token punctuation">)</span> 的 <span class="token number">4D</span> 张量。输出的行列数可能会因为填充方法而改变。
</code></pre>
      </div>
<ol start="7">
<li>
<p>池化层
MaxPooling1D 层
对时域 1D 信号进行最大值池化。</p>
<div class="gatsby-highlight">
      <pre class="language-python"><code>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>pooling<span class="token punctuation">.</span>MaxPooling1D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">'valid'</span><span class="token punctuation">)</span>
pool_size：整数，池化窗口大小
strides：整数或 <span class="token boolean">None</span>，下采样因子，例如设 <span class="token number">2</span> 将会使得输出 shape 为输入的一半，若为 <span class="token boolean">None</span> 则默认值为pool_size。
padding：“valid”或者“same”
该层输入 shape 为形如 <span class="token punctuation">(</span>samples，steps，features<span class="token punctuation">)</span> 的 <span class="token number">3D</span> 张量，输出 shape 为形如 <span class="token punctuation">(</span>samples，downsampled_steps，features<span class="token punctuation">)</span> 的 <span class="token number">3D</span> 张量。
</code></pre>
      </div>
</li>
</ol>
<p>MaxPooling2D 层
为空域信号施加最大值池化。</p>
<p>keras.layers.pooling.MaxPooling2D(pool<em>size=(2, 2), strides=None, padding='valid', data</em>format=None)
pool<em>size：整数或长为 2 的整数 tuple，代表在两个方向（竖直，水平）上的下采样因子，如取 (2，2) 将使图片在两个维度上均变为原长的一半。为整数意为各个维度值相同且为该数字。
strides：整数或长为 2 的整数 tuple，或者 None，步长值。
border</em>mode：“valid”或者“same”
data<em>format：字符串，“channels</em>first”或“channels<em>last”之一，代表图像的通道维的位置。以 128×128 的 RGB 图像为例，“channels</em>first”应将数据组织为 (3,128,128)，而“channels<em>last”应将数据组织为 (128,128,3)。该参数的默认值是 ~/.keras/keras.json 中设置的值，若从未设置过，则为“channels</em>last”。
该层输入 shape 在“channels<em>first”模式下，为形如 (samples，channels, rows，cols) 的 4D 张量，在“channels</em>last”模式下，为形如 (samples，rows, cols，channels) 的 4D 张量。输出 shape 在“channels<em>first”模式下，为形如 (samples，channels, pooled</em>rows, pooled<em>cols) 的 4D 张量，在“channels</em>last”模式下，为形如 (samples，pooled<em>rows, pooled</em>cols，channels) 的4D张量。</p>
<p>AveragePooling1D 层
对时域 1D 信号进行平均值池化。</p>
<p>keras.layers.pooling.AveragePooling1D(pool<em>size=2, strides=None, padding='valid')
pool</em>size：整数，池化窗口大小
strides：整数或 None，下采样因子，例如设 2 将会使得输出 shape 为输入的一半，若为 None 则默认值为 pool<em>size。
padding：“valid”或者“same”
该层输入 shape 为形如 (samples，steps，features) 的 3D 张量，输出 shape 为形如 (samples，downsampled</em>steps，features) 的 3D 张量。</p>
<p>AveragePooling2D 层
为空域信号施加平均值池化。</p>
<p>keras.layers.pooling.AveragePooling2D(pool<em>size=(2, 2), strides=None, padding='valid', data</em>format=None)
pool<em>size：整数或长为 2 的整数 tuple，代表在两个方向（竖直，水平）上的下采样因子，如取 (2，2) 将使图片在两个维度上均变为原长的一半。为整数意为各个维度值相同且为该数字。
strides：整数或长为 2 的整数tuple，或者 None，步长值。
border</em>mode：“valid”或者“same”
data<em>format：字符串，“channels</em>first”或“channels<em>last”之一，代表图像的通道维的位置。以 128×128 的 RGB 图像为例，“channels</em>first”应将数据组织为 (3,128,128)，而“channels<em>last”应将数据组织为 (128,128,3)。该参数的默认值是 ~/.keras/keras.json 中设置的值，若从未设置过，则为“channels</em>last”。
该层输入 shape 在“channels<em>first”模式下，为形如 (samples，channels, rows，cols) 的 4D 张量，在“channels</em>last”模式下，为形如 (samples，rows, cols，channels) 的 4D 张量。输出 shape 在“channels<em>first”模式下，为形如 (samples，channels, pooled</em>rows, pooled<em>cols) 的 4D 张量，在“channels</em>last”模式下，为形如 (samples，pooled<em>rows, pooled</em>cols，channels) 的 4D 张量。</p>
<p>GlobalMaxPooling1D 层
对于时间信号的全局最大池化。</p>
<p>keras.layers.pooling.GlobalMaxPooling1D()
该层输入 shape 为形如 (samples，steps，features) 的 3D 张量，输出 shape 为形如 (samples, features) 的 2D 张量。</p>
<p>GlobalAveragePooling1D 层
为时域信号施加全局平均值池化。</p>
<p>keras.layers.pooling.GlobalAveragePooling1D()
该层输入 shape 为形如 (samples，steps，features) 的 3D 张量，输出 shape 为形如 (samples, features) 的 2D 张量。</p>
<p>GlobalMaxPooling2D 层
为空域信号施加全局最大值池化。</p>
<p>keras.layers.pooling.GlobalMaxPooling2D(data<em>format=None)
data</em>format：字符串，“channels<em>first”或“channels</em>last”之一，代表图像的通道维的位置。以 128×128 的 RGB 图像为例，“channels<em>first”应将数据组织为 (3,128,128)，而“channels</em>last”应将数据组织为 (128,128,3)。该参数的默认值是 ~/.keras/keras.json 中设置的值，若从未设置过，则为“channels<em>last”。
该层输入 shape 在“channels</em>first”模式下，为形如 (samples，channels, rows，cols) 的 4D 张量，在“channels<em>last”模式下，为形如 (samples，rows, cols，channels) 的 4D 张量。输出 shape 为形如 (nb</em>samples, channels) 的 2D 张量。</p>
<p>GlobalAveragePooling2D 层
为空域信号施加全局平均值池化。</p>
<p>keras.layers.pooling.GlobalAveragePooling2D(data<em>format=None)
data</em>format：字符串，“channels<em>first”或“channels</em>last”之一，代表图像的通道维的位置。以 128×128 的 RGB 图像为例，“channels<em>first”应将数据组织为 (3,128,128)，而“channels</em>last”应将数据组织为 (128,128,3)。该参数的默认值是 ~/.keras/keras.json 中设置的值，若从未设置过，则为“channels<em>last”。
该层输入 shape 在“channels</em>first”模式下，为形如 (samples，channels, rows，cols) 的 4D 张量，在“channels<em>last”模式下，为形如 (samples，rows, cols，channels) 的 4D 张量。输出 shape 为形如 (nb</em>samples, channels) 的 2D 张量。</p>
<div class="gatsby-highlight">
      <pre class="language-none"><code>8. 循环层
Recurrent 层
这是循环层的抽象类，请不要在模型中直接应用该层（因为它是抽象类，无法实例化任何对象）。请使用它的子类 LSTM、GRU 或 SimpleRNN，这些循环层都服从本层的性质，并接受本层指定的所有关键字参数。
```python
keras.layers.recurrent.Recurrent(return_sequences=False, go_backwards=False, stateful=False)
return_sequences：布尔值，默认 False，控制返回类型。若为 True 则返回整个序列，否则仅返回输出序列的最后一个输出
go_backwards：布尔值，默认为 False，若为 True，则逆向处理输入序列并返回逆序后的序列
stateful：布尔值，默认为 False，若为 True，则一个 batch 中下标为 i 的样本的最终状态将会用作下一个 batch 同样下标的样本的初始状态
input_dim：输入维度，当使用该层为模型首层时，应指定该值（或等价的指定 input_shape)
input_length：当输入序列的长度固定时，该参数为输入序列的长度。当需要在该层后连接 Flatten 层，然后又要连接 Dense 层时，需要指定该参数，否则全连接的输出无法计算出来。注意，如果循环层不是网络的第一层，你需要在网络的第一层中指定序列的长度（通过 input_shap 指 定）。
该层输入 shape 为形如 (samples，timesteps，input_dim) 的 3D 张量。输出 shape 如果 return_sequences=True 则返回形如 (samples，timesteps，output_dim) 的 3D 张量，否则返回形如 (samples，output_dim) 的 2D 张量。

# 作为Sequential模型的第一层
model = Sequential()
model.add(LSTM(32, input_shape=(10, 64)))
# model.output_shape == (None, 32)
# 注意: `None` 是batch维.

# 下面是完全相同的:
model = Sequential()
model.add(LSTM(32, input_dim=64, input_length=10))

# 作为中间层, 不需要再去定义输入大小:
model.add(LSTM(16))

# 要堆叠循环层，必须设置输出到其他循环层的循环层 return_sequences=True
# 注意你只需在第一层上定义输入大小
model = Sequential()
model.add(LSTM(64, input_dim=64, input_length=10, return_sequences=True))
model.add(LSTM(32, return_sequences=True))
model.add(LSTM(10))
循环层支持通过时间步变量对输入数据进行 Masking，如果想将输入数据的一部分屏蔽掉，请使用 Embedding 层并将参数 mask_zero 设为 True。

可以将 RNN 设置为“stateful”，意味着由每个 batch 计算出的状态都会被重用于初始化下一个 batch 的初始状态。状态 RNN 假设连续的两个 batch 之中，相同下标的元素有一一映射关系。要启用状态 RNN，请在实例化层对象时指定参数 stateful=True，并在 Sequential 模型使用固定大小的 batch：通过在模型的第一层传入 batch_size=(...) 和 input_shape 来实现。在函数式模型中，对所有的输入都要指定相同的 batch_size。</code></pre>
      </div>
<p>SimpleRNN 层
全连接 RNN 网络，RNN 的输出会被回馈到输入。</p>
<div class="gatsby-highlight">
      <pre class="language-python"><code>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>recurrent<span class="token punctuation">.</span>SimpleRNN<span class="token punctuation">(</span>units<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'tanh'</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> recurrent_dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span>
units：输出维度
activation：激活函数，为预定义的激活函数名
dropout：<span class="token number">0</span><span class="token operator">~</span><span class="token number">1</span> 之间的浮点数，控制输入线性变换的神经元断开比例
recurrent_dropout：<span class="token number">0</span><span class="token operator">~</span><span class="token number">1</span> 之间的浮点数，控制循环状态的线性变换的神经元断开比例
GRU 层
门限循环单元。

keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>recurrent<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>units<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'tanh'</span><span class="token punctuation">,</span> recurrent_activation<span class="token operator">=</span><span class="token string">'hard_sigmoid'</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> recurrent_dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span>
units：输出维度
activation：激活函数，为预定义的激活函数名
recurrent_activation<span class="token punctuation">:</span> 为循环步施加的激活函数
dropout：<span class="token number">0</span><span class="token operator">~</span><span class="token number">1</span> 之间的浮点数，控制输入线性变换的神经元断开比例
recurrent_dropout：<span class="token number">0</span><span class="token operator">~</span><span class="token number">1</span> 之间的浮点数，控制循环状态的线性变换的神经元断开比例
LSTM 层
Keras 长短期记忆模型。

keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>recurrent<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>units<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'tanh'</span><span class="token punctuation">,</span> recurrent_activation<span class="token operator">=</span><span class="token string">'hard_sigmoid'</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> recurrent_dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span>
units：输出维度
activation：激活函数，为预定义的激活函数名
recurrent_activation<span class="token punctuation">:</span> 为循环步施加的激活函数
dropout：<span class="token number">0</span><span class="token operator">~</span><span class="token number">1</span> 之间的浮点数，控制输入线性变换的神经元断开比例
recurrent_dropout：<span class="token number">0</span><span class="token operator">~</span><span class="token number">1</span> 之间的浮点数，控制循环状态的线性变换的神经元断开比例
</code></pre>
      </div>
<ol start="9">
<li>
<p>嵌入层
Embedding 层
嵌入层将正整数（下标）转换为具有固定大小的向量，如 [[4],[20]]->[[0.25,0.1],[0.6,-0.2]]。Embedding 层只能作为模型的第一层。</p>
<div class="gatsby-highlight">
      <pre class="language-python"><code>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span> mask_zero<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> input_length<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
input_dim：大或等于 <span class="token number">0</span> 的整数，字典长度
output_dim：大于 <span class="token number">0</span> 的整数，代表全连接嵌入的维度
mask_zero：布尔值，确定是否将输入中的‘<span class="token number">0</span>’看作是应该被忽略的‘填充’（padding）值，该参数在使用循环层处理变长输入时有用。设置为 <span class="token boolean">True</span> 的话，模型中后续的层必须都支持 masking，否则会抛出异常。如果该值为 <span class="token boolean">True</span>，则下标 <span class="token number">0</span> 在字典中不可用，input_dim 应设置为 <span class="token operator">|</span>vocabulary<span class="token operator">|</span> <span class="token operator">+</span> <span class="token number">2</span>。
input_length：当输入序列的长度固定时，该值为其长度。如果要在该层后接 Flatten 层，然后接 Dense 层，则必须指定该参数，否则 Dense 层的输出维度无法自动推断。
该层输入 shape 为形如 <span class="token punctuation">(</span>samples，sequence_length<span class="token punctuation">)</span> 的 <span class="token number">2D</span> 张量，输出 shape 为形如 <span class="token punctuation">(</span>samples<span class="token punctuation">,</span> sequence_length<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span> 的 <span class="token number">3D</span> 张量。
</code></pre>
      </div>
</li>
</ol>
<p>model = Sequential()
model.add(Embedding(1000, 64, input_length=10))</p>
<h1>模型会接收一个(batch, input_length)的整数矩阵.</h1>
<h1>输入中的最大的整数 (词语索引) 应该不大于 999 (词表大小).</h1>
<h1>model.output_shape == (None, 10, 64), None是batch维.</h1>
<p>input_array = np.random.randint(1000, size=(32, 10))</p>
<p>model.compile('rmsprop', 'mse')
output<em>array = model.predict(input</em>array)
assert output_array.shape == (32, 10, 64)</p>
<div class="gatsby-highlight">
      <pre class="language-none"><code>10. 融合层
Merge 层提供了一系列用于融合两个层或两个张量的层对象和方法。以大写首字母开头的是 Layer 类，以小写字母开头的是张量的函数。小写字母开头的张量函数在内部实际上是调用了大写字母开头的层。

Add 层
该层接收一个列表的同 shape 张量，并返回它们的和，shape 不变。

keras.layers.merge.Add()
该层的函数式包装为：add(inputs)，inputs 为长度至少为 2 的张量列表。

Multiply 层
该层接收一个列表的同 shape 张量，并返回它们的逐元素积的张量，shape 不变。

keras.layers.merge.Multiply()
该层的函数式包装为：multiply(inputs)，inputs 为长度至少为 2 的张量列表。

Average 层
该层接收一个列表的同 shape 张量，并返回它们的逐元素均值，shape 不变。

keras.layers.merge.Average()
该层的函数式包装为：average(inputs)，inputs 为长度至少为 2 的张量列表。

Maximum 层
该层接收一个列表的同 shape 张量，并返回它们的逐元素最大值，shape 不变。

keras.layers.merge.Maximum()
该层的函数式包装为：maximum(inputs)，inputs 为长度至少为 2 的张量列表。

Concatenate 层
该层接收一个列表的同 shape 张量，并返回它们的按照给定轴相接构成的向量。

keras.layers.merge.Concatenate(axis=-1)
axis: 想接的轴
该层的函数式包装为：concatenate(inputs, axis=-1))，inputs 为长度至少为2的张量列，axis 为相接的轴。

Dot 层
计算两个 tensor 中样本的张量乘积。例如，如果两个张量 a 和 b 的 shape 都为 (batch_size, n)，则输出为形如 (batch_size,1) 的张量，结果张量每个 batch 的数据都是 a[i,:] 和 b[i,:] 的矩阵（向量）点积。

keras.layers.merge.Dot(axes, normalize=False)
axes: 整数或整数的 tuple，执行乘法的轴。
normalize: 布尔值，是否沿执行成绩的轴做 L2 规范化，如果设为 True，那么乘积的输出是两个样本的余弦相似性。
该层的函数式包装为：dot(inputs, axes, normalize=False)。inputs 为长度至少为2的张量列；axes 为整数或整数的 tuple，执行乘法的轴；normalize 为布尔值，是否沿执行成绩的轴做L2规范化，如果设为True，那么乘积的输出是两个样本的余弦相似性。</code></pre>
      </div></div><div class="sc-ifAKCX jorVQO" data-reactid="10"><div data-reactid="11"><a href="/myblog/posts/9" data-reactid="12">Prev</a></div><div data-reactid="13"><a href="/myblog/posts/11" data-reactid="14">Next</a></div></div></div></div></div></div></div></body></html>