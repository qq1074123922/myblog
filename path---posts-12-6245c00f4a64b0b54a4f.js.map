{"version":3,"sources":["webpack:///path---posts-12-6245c00f4a64b0b54a4f.js","webpack:///./.cache/json/posts-12.json"],"names":["webpackJsonp","361","module","exports","data","markdownRemark","html","indexJson","title","pathContext","slug","pid","next","prev"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,gBAAkBC,KAAA,osTAA0sTC,WAAcC,MAAA,YAAmBC,aAAgBC,KAAA,aAAAC,IAAA,GAAAC,KAAA,KAAAC,KAAA","file":"path---posts-12-6245c00f4a64b0b54a4f.js","sourcesContent":["webpackJsonp([279941687692024],{\n\n/***/ 361:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"markdownRemark\":{\"html\":\"<h4>深度学习术语对于新手来说可能是相当的压倒性的。本词汇表试图定义常用术语，并链接到原始参考资料和其他资源，以帮助读者更深入地了解特定主题。</h4>\\n<h4>什么是深度学习与“一般”机器学习术语之间的界限是相当模糊的。我正在努力保留深度学习的术语表，但是这些决定有些武断。例如，我在这里不包括“交叉验证”，因为这是一种通用的技术，可以在机器学习中使用。不过，我已经决定加入softmax或word2vec等词语，因为它们通常与深度学习相关联，即使它们不是深度学习技术。</h4>\\n<h4>激活函数</h4>\\n<p>为了允许神经网络学习复杂的决策边界，我们对其某些层应用非线性激活函数。常用的功能包括sigmoid，tanh，ReLU（整流线性单位）及其变体。</p>\\n<h4>Adadelta</h4>\\n<p>Adadelta是基于梯度下降的学习算法，适应随时间变化的每个参数的学习速率。它被认为是对Adagrad的一种改进，Adagrad对超参数更为敏感，并可能过于积极地降低学习速度。Adadelta它与rmsprop类似，可以用来代替vanilla SGD。</p>\\n<h4>ADADELTA：一种自适应学习速率方法</h4>\\n<p>斯坦福CS231n：优化算法</p>\\n<p>梯度下降优化算法的概述</p>\\n<h4>Adagrad</h4>\\n<p>Adagrad是一种自适应学习速率算法，能够随时间跟踪平方梯度并自动调整每个参数的学习速率。可以使用它来代替vanilla SGD，对于稀疏数据尤其有用，它将更高的学习率分配给不经常更新的参数。</p>\\n<p>在线学习和随机优化的自适应次梯度方法</p>\\n<p>斯坦福CS231n：优化算法</p>\\n<p>梯度下降优化算法的概述</p>\\n<h4>亚当</h4>\\n<p>Adam是一种类似于rmsprop的自适应学习速率算法，但更新是\\n直接使用梯度的第一个和第二个时刻的移动平均值进行估计的，并且还包括偏差修正项。</p>\\n<p>亚当：随机优化的一种方法</p>\\n<p>梯度下降优化算法的概述</p>\\n<h4>Affine Layer</h4>\\n<p>神经网络中的完全连接层。仿射意味着前一层中的每个神经元都连接到当前层中的每个神经元。在很多方面，这是神经网络的“标准”层。在进行最终预测之前，仿射层通常被添加在卷积神经网络或递归神经网络的输出之上。仿射层通常形式的y = f(Wx + b)，其中x是该层的输入，W参数，b偏置向量，并f非线性激活函数</p>\\n<h4>Attention Mechanism</h4>\\n<p>注意机制受到人类视觉注意力的启发，能够专注于图像的特定部分。注意机制可以结合在语言处理和图像识别两种架构中，以帮助网络在进行预测时学习“关注”什么。</p>\\n<p>注意力和记忆深度学习和NLP</p>\\n<h4>Alexnet</h4>\\n<p>Alexnet是卷积神经网络体系结构的名称，它大大赢得了ILSVRC 2012大赛的冠军，并负责CNN在图像识别领域的兴趣。它由5个卷积层组成，其中一些是最大池层，另外三层是完全连接层，最后是1000个软最大值。Alexnet是在深度卷积神经网络的ImageNet分类中引入的。</p>\\n<h4>Autoencoder</h4>\\n<p>自动编码器是一个神经网络模型，其目标是预测输入本身，通常是通过网络某处的“瓶颈”。通过引入一个瓶颈，我们迫使网络学习输入的低维表示，有效地将输入压缩成一个良好的表示。自动编码器与PCA和其他降维技术相关，但由于它们的非线性本质，可以学习更复杂的映射。存在广泛的自编码器体系结构，包括去噪自动编码器，变化自动编码器或序列自动编码器。</p>\\n<h4>Average-Pooling</h4>\\n<p>平均汇集是用于图像识别的卷积神经网络中的汇集技术。它的工作原理是通过在像素之类的特征块上滑动窗口，并取得窗口内所有值的平均值。它将输入表示压缩为低维表示。</p>\\n<h4>Backpropagation</h4>\\n<p>反向传播是有效地计算神经网络中的梯度的算法，或更一般地说是前馈计算图。归结为从网络输出开始应用分化的链式规则，并向后传播梯度。反向传播的第一个用途可以追溯到20世纪60年代的Vapnik，但是通过反向传播错误的学习表示经常被引用为来源。</p>\\n<p>计算图演算：反向传播</p>\\n<h4>反向传播时间（BPTT）</h4>\\n<p>反向传播时间（纸）是应用于递归神经网络（RNN）的反向传播算法。BPTT可以被看作是应用于RNN的标准反向传播算法，其中每个时间步代表一个层，参数在层间共享。由于RNN在所有时间步骤中共享相同的参数，因此一次性步骤中的错误必须“全部”时间地反向传播到所有以前的时间步骤，因此也就是名称。处理长序列（数百个输入）时，经常使用截断版本的BPTT来降低计算成本。截断的BPTT会在固定数量的步骤后停止反向传播错误。</p>\\n<p>反向传播的时间：它做什么和如何做到这一点</p>\\n<h4>Batch Normalization</h4>\\n<p>批量标准化是一种技术，使每个最小批量的层输入标准化。它加快培训，允许使用更高的学习率，并可以充当正规化者。批量归一化对于卷积和前馈神经网络是非常有效的，但还没有被成功应用于递归神经网络。</p>\\n<p>批量标准化：通过减少内部协变量加速深度网络培训</p>\\n<p>批量归一化递归神经网络</p>\\n<h4>Bidirectional RNN</h4>\\n<p>双向递归神经网络是一种包含两个不同方向的RNN的神经网络。前向RNN从开始到结束读取输入序列，而后向RNN从结束到开始读取输入序列。两个RNN堆叠在一起，通常通过附加两个向量来组合它们的状态。双向RNN经常被用在自然语言问题中，在进行预测之前，我们要考虑单词前后的上下文。</p>\\n<p>双向递归神经网络</p>\\n<h4>Caffe</h4>\\n<p>Caffe是由伯克利视觉和学习中心开发的深度学习框架。Caffe对于视觉任务和CNN模型特别受欢迎和表现。</p>\\n<h4>Categorical Cross-Entropy Loss</h4>\\n<p>分类交叉熵损失也被称为负对数似然值。这是一个流行的分类问题的损失函数，并测量两个概率分布之间的相似性，通常是真实的标签和预测的标签。它由下式给出L = -sum(y * log(y<em>prediction))其中y为真标签的概率分布（通常是一个热载体）和y</em>prediction是预测标签的概率分布，通常从一个未来SOFTMAX。</p>\\n<h4>Channel</h4>\\n<p>向深度学习模型输入数据可以有多个通道。规范示例是具有红色，绿色和蓝色通道的图像。图像可以被表示为具有与通道，高度和宽度相对应的尺寸的三维张量。自然语言数据也可以具有多个通道，例如以不同类型的嵌入的形式。</p>\\n<h4>Convolutional Neural Network (CNN, ConvNet)</h4>\\n<p>CNN使用卷积来连接输入的局部区域的提取特征。大多数CNN包含卷积，合并和仿射层的组合。有线电视网络已经获得了普及，尤其是通过在视觉识别任务方面的卓越表现，他们已经在几年时间里设定了艺术水平。</p>\\n<p>斯坦福CS231n类 - 用于视觉识别的卷积神经网络</p>\\n<p>了解用于NLP的卷积神经网络</p>\\n<h4>Deep Belief Network (DBN)</h4>\\n<p>DBN是一种概率图形模型，以无监督的方式学习数据的分层表示。DBN由多个隐藏层组成，每个连续层对中的神经元之间都有连接。DBN是通过将多个RBN堆叠在一起并逐一进行培训而建立的。</p>\\n<p>一种深度信仰网络的快速学习算法</p>\\n<h4>Deep Dream</h4>\\n<p>谷歌发明的一种技术，试图提炼深层卷积神经网络所捕获的知识。该技术可以生成新的图像，或者转换现有的图像，并赋予它们梦幻般的味道，尤其是当递归地应用时。</p>\\n<p>在Github上的深刻的梦想</p>\\n<p>启示主义：深入到神经网络</p>\\n<h4>Dropout</h4>\\n<p>辍学是防止过度拟合的神经网络正则化技术。它通过在每次训练迭代中随机地将它们的一部分设置为0来防止神经元进行协调。丢失可以用各种方式来解释，例如从指数的不同网络随机抽样。Dropout层首先通过在CNN中的使用而获得了普及，但是之后已经被应用于其他层，包括输入嵌入或循环网络。</p>\\n<p>辍学：防止神经网络过度配置的一种简单方法</p>\\n<p>递归神经网络正则化</p>\\n<h4>Embedding</h4>\\n<p>嵌入将输入表示（如单词或句子）映射到向量中。一个流行的嵌入类型是词嵌入，如word2vec或GloVe。我们也可以嵌入句子，段落或图像。例如，通过将图像及其文字描述映射到一个共同的嵌入空间，并将它们之间的距离最小化，我们可以将标签与图像进行匹配。可以明确地学习嵌入，例如在word2vec中，或作为受监督任务的一部分，如情感分析。通常，网络的输入层用预先训练好的嵌入进行初始化，然后对其进行微调以适应当前的任务。</p>\\n<h4>Exploding Gradient Problem</h4>\\n<p>爆炸梯度问题与消失梯度问题相反。在深度神经网络中，反向传播期间梯度可能会发生爆炸，导致数字溢出。处理爆炸渐变的常用技术是执行渐变裁剪。</p>\\n<p>训练递归神经网络的难点</p>\\n<h4>Fine-Tuning</h4>\\n<p>微调是指利用来自其他任务的参数（例如无监督的训练任务）来初始化网络，然后基于当前任务更新这些参数的技术。例如，NLP体系结构通常使用像word2vec这样的预先训练的词嵌入，然后根据特定任务（如情感分析）在训练期间更新这些词嵌入。</p>\\n<h4>Gradient Clipping</h4>\\n<p>梯度剪切是一种防止在深度网络（通常是递归神经网络）中发生梯度变化的技术。梯度裁剪存在多种方式，但常用的方法是当L2范数超过一定阈值时，对参数矢量的梯度进行归一化new<em>gradients = gradients * threshold / l2</em>norm(gradients)。</p>\\n<p>训练递归神经网络的难点</p>\\n<h4>GloVe</h4>\\n<p>GloVe是一种无监督学习算法，用于获取单词的矢量表示（嵌入）。Glove矢量与word2vec具有相同的用途，但由于在同现统计方面进行了训练，因此具有不同的矢量表示。</p>\\n<p>GloVe：词表示的全局向量</p>\\n<h4>GoogleLeNet</h4>\\n<p>赢得2014年ILSVRC挑战的卷积神经网络架构的名称。网络使用Inception模块来减少参数，提高网络内部计算资源的利用率。</p>\\n<p>越来越深入的卷积</p>\\n<h4>GRU</h4>\\n<p>门控重复单元是一个参数较少的LSTM单元的简化版本。就像LSTM单元一样，它使用门控机制来允许RNN通过防止消失梯度问题来有效地学习远程依赖。GRU由一个复位和更新门组成，用于确定旧存储器的哪一部分保持与当前时间步的新值更新。</p>\\n<p>使用RNN编码器 - 解码器来学习用于统计机器翻译的短语表示</p>\\n<p>复发神经网络教程，第4部分 - 用Python和Theano实现GRU / LSTM RNN</p>\\n<h4>Highway Layer</h4>\\n<p>高速公路层（纸）是一种神经网络层，它使用门控机制来控制通过层的信息流。堆叠多个公路层允许训练非常深的网络。高速公路层通过学习一个选通功能来工作，该选通功能选择输入的哪些部分通过，哪些部分通过变换功能，例如标准的仿射层。高速公路层的基本表述是T * h(x) + (1 - T) * x，其中T值为0和1之间的学习门控函数h(x)是任意的输入变换并且x是输入。请注意，所有这些都必须具有相同的大小。</p>\\n<h4>ICML</h4>\\n<p>该国际会议的机器学习，一个顶级的机器学习会议。</p>\\n<h4>ILSVRC</h4>\\n<p>所述ImageNet大型视觉辨识挑战在大规模的计算结果为对象检测和图像分类算法。这是计算机视觉领域最受欢迎的学术挑战。在过去的几年里，深度学习技术使错误率从30％显着降低到不到5％，在几个分类任务中击败了人的表现。</p>\\n<h4>Inception Module</h4>\\n<p>启发模块被用于卷积神经网络，以允许更有效的计算和更深的网络通过叠加1×1卷积的维度降低。</p>\\n<p>越来越深入的卷积</p>\\n<h4>Keras</h4>\\n<p>Kears是一个基于Python的深度学习库，包含许多用于深度神经网络的高级构建模块。它可以在TensorFlow，Theano或CNTK之上运行。</p>\\n<h4>LSTM</h4>\\n<p>通过使用记忆门控机制，发明了长期短期记忆网络来防止递归神经网络中的消失梯度问题。使用LSTM单元计算RNN中的隐藏状态，我们帮助网络高效地传播梯度和学习远程依赖性。</p>\\n<p>长期的短期记忆</p>\\n<p>了解LSTM网络</p>\\n<p>复发神经网络教程，第4部分 - 用Python和Theano实现GRU / LSTM RNN</p>\\n<h4>Max-Pooling</h4>\\n<p>一池操作通常在卷积神经网络中使用。最大池图层从一个特征块中选择最大值。就像卷积图层一样，通过窗口（补丁）大小和跨度大小来对池化层进行参数化。例如，我们可以使用步长大小2在10×10特征矩阵上滑动大小为2×2的窗口，在每个窗口内的所有4个值中选择最大值，从而产生新的5×5特征矩阵。合并层通过仅保留最显着的信息来帮助减少表示的维度，并且在图像输入的情况下，它们为翻译提供基本的不变性（即使图像被移动几个像素，也将选择相同的最大值）。汇聚层通常插入连续的卷积层之间。</p>\\n<h4>MNIST</h4>\\n<p>该MNIST数据集是也许是最常用的图像识别数据集。它包括6万个训练和10,000个手写数字的测试例子。每个图像是28×28像素大。现有技术模型通常在测试集上达到99.5％或更高的精确度。</p>\\n<h4>Momentum</h4>\\n<p>动量是梯度下降算法的扩展，可以加速或抑制参数更新。在实践中，包括渐变下降更新中的动量项导致深度网络中更好的收敛速率。</p>\\n<p>通过向后传播错误学习表示</p>\\n<h4>Multilayer Perceptron (MLP(</h4>\\n<p>多层感知器是具有多个完全连接层的前馈神经网络，其使用非线性激活函数来处理不能线性分离的数据。MLP是多层神经网络的最基本的形式，或者如果它具有超过两层的话，则是深度神经网络。</p>\\n<h4>Negative Log Likelihood (NLL)</h4>\\n<p>请参阅分类交叉熵损失。</p>\\n<h4>Neural Machine Translation (NMT)</h4>\\n<p>NMT系统使用神经网络在诸如英语和法语等语言之间进行翻译。可以使用双语语料库对NMT系统进行端到端的培训，这与需要手工特征和工程设计的传统机器翻译系统不同。NMT系统通常使用编码器和解码器递归神经网络来实现，其分别对源句子进行编码并产生目标句子。</p>\\n<p>用神经网络进行序列学习的顺序</p>\\n<p>使用RNN编码器 - 解码器来学习用于统计机器翻译的短语表示</p>\\n<h4>Neural Turing Machine (NTM)</h4>\\n<p>NMT是可以从例子推断简单算法的神经网络架构。例如，NTM可以通过示例输入和输出来学习排序算法。NTM通常学习某种形式的记忆和注意机制来处理程序执行期间的状态。</p>\\n<p>神经图灵机</p>\\n<h4>非线性Nonlinearity</h4>\\n<p>请参阅激活功能。</p>\\n<h4>Noise-contrastive estimation (NCE)</h4>\\n<p>噪声对比估计是通常用于训练具有大量输出词汇的分类器的抽样损失。计算大量可能的类的softmax是非常昂贵的。使用NCE，我们可以通过训练分类器来区分样本与“真实”分布和人工生成的噪声分布，从而将问题降低到二元分类问题。</p>\\n<p>噪声对比估计：非标准统计模型的一种新的估计原理</p>\\n<p>利用噪声对比估计有效地学习单词嵌入</p>\\n<h4>Pooling</h4>\\n<p>请参阅最大池或平均池。</p>\\n<h4>Restricted Boltzmann Machine (RBN)</h4>\\n<p>RBM是一种概率图模型，可以被解释为一个随机的人工神经网络。RBN以无监督的方式学习数据的表示。RBN由可见和隐藏层以及这些层中每个层中的二元神经元之间的连接组成。RBN可以使用对比发散（梯度下降的近似）进行有效训练。</p>\\n<p>第六章：动力系统中的信息处理：和谐理论的基础</p>\\n<p>限制玻尔兹曼机介绍</p>\\n<h4>递归神经网络（RNN）</h4>\\n<p>RNN通过隐藏状态或存储器模拟顺序交互。它可以占用N个输入并产生多达N个输出。例如，输入序列可以是输出是每个词（N到N）的词性标签的句子。输入可以是一个句子，并且输出一个句子的情感分类（N-to-1）。输入可以是单个图像，并且输出可以是对应于图像描述（1到N）的一系列单词。在每个时间步，RNN基于当前输入和先前的隐藏状态计算新的隐藏状态（“记忆”）。“经常性”来源于这样一个事实，即在每个步骤中使用相同的参数，网络根据不同的输入执行相同的计算。</p>\\n<p>了解LSTM网络</p>\\n<p>复发神经网络教程，第1部分 - RNN简介</p>\\n<p>递归神经网络\\n递归神经网络的泛化回归神经网络的树状结构。在每个递归中应用相同的权重。就像RNN一样，递归神经网络可以使用反向传播进行端对端训练。尽管可以将树结构学习为优化问题的一部分，但是递归神经网络通常应用于已经具有预定义结构的问题，如自然语言处理中的分析树。</p>\\n<p>用递归神经网络解析自然场景和自然语言</p>\\n<h4>RELU</h4>\\n<p>整流线性单位的缩写。在深度神经网络中，ReLUs经常用作激活功能。他们被定义f(x) = max(0, x)。ReLU相对于函数的优点tanh包括它们往往是稀疏的（它们的激活容易被设置为0），并且它们受渐变问题的影响较小。ReLUs是卷积神经网络中最常用的激活函数。存在多种ReLU的变体，例如泄漏ReLU，参数ReLU（PReLU）或更平滑的Softplus近似。</p>\\n<p>深入整流器：在ImageNet分类上超越人类级别的性能</p>\\n<p>整流器非线性改善神经网络声学模型</p>\\n<p>整流线性单元改进受限玻尔兹曼机器</p>\\n<h4>RESNET</h4>\\n<p>Deep Residual Networks赢得了2015年ILSVRC挑战赛。这些网络通过在堆栈层之间引入快捷连接起作用，允许优化器学习“更容易”的残差映射，而不是更复杂的原始映射。这些快捷连接与高速公路层相似，但它们与数据无关，不会引入其他参数或培训复杂性。ResNet在ImageNet测试集中达到了3.57％的错误率。</p>\\n<p>用于图像识别的深度残差学习</p>\\n<h4>RMSProp</h4>\\n<p>RMSProp是一个基于梯度的优化算法。它与Adagrad类似，但引入了一个额外的衰减术语来抵消Adagrad的学习速度的迅速下降。</p>\\n<p>机器学习的神经网络讲座6a</p>\\n<p>斯坦福CS231n：优化算法</p>\\n<p>梯度下降优化算法的概述</p>\\n<h4>Seq2Seq</h4>\\n<p>序列 - 序列模型读取序列（如句子）作为输入，并生成另一个序列作为输出。它与标准的RNN不同之处在于，在网络开始产生任何输出之前，输入序列被完全读取。通常情况下，seq2seq模型使用两个RNN来实现，作为编码器和解码器。神经机器翻译是一个seq2seq模型的典型例子。</p>\\n<p>用神经网络进行序列学习的顺序</p>\\n<h4>SGD</h4>\\n<p>随机梯度下降（Wikipedia）是一种基于梯度的优化算法，用于在训练阶段学习网络参数。通常使用反向传播算法计算梯度。在实践中，人们使用SGD的minibatch版本，其中参数更新是基于批处理而不是单个示例执行的，从而提高了计算效率。存在许多对香草SGD的扩展，包括Momentum，Adagrad，rmsprop，Adadelta或Adam。</p>\\n<p>在线学习和随机优化的自适应次梯度方法</p>\\n<p>斯坦福CS231n：优化算法</p>\\n<p>梯度下降优化算法的概述</p>\\n<h4>SOFTMAX</h4>\\n<p>该SOFTMAX功能通常用于原始分数的向量转换成类概率在一个神经网络用于分类的输出层。它通过指数归一化和归一化常数除以归一化得分。如果我们正在处理大量的类，例如机器翻译中的大量词汇，那么归一化常数计算起来就很昂贵。存在使计算更有效的各种备选方案，包括分层Softmax或使用诸如NCE的基于抽样的损失。</p>\\n<h4>TensorFlow</h4>\\n<p>TensorFlow是一个开源的C ++ / Python软件库，用于使用数据流图进行数值计算，特别是深度神经网络。它是由Google创建的。在设计上，它与Theano最为相似，比Caffe或Keras更低。</p>\\n<h4>Theano</h4>\\n<p>Theano是一个Python库，允许您定义，优化和评估数学表达式。它包含许多用于深度神经网络的构件。Theano是一个类似于Tensorflow的低级库。更高级的图书馆包括Keras和Caffe。</p>\\n<h4>Vanishing Gradient Problem</h4>\\n<p>消失梯度问题出现在深度非常深的神经网络中，通常为递归神经网络，它使用梯度趋于较小（范围从0到1）的激活函数。由于这些小梯度在反向传播过程中倍增，因此它们倾向于在整个层中“消失”，从而阻止网络学习远程依赖性。解决这个问题的常用方法是使用像ReLU这样的激活函数，这些函数不会遇到小的梯度，或者使用像LSTM这样的体系结构来明确消除渐变梯度。这个问题的反面被称为爆炸的梯度问题。</p>\\n<p>训练递归神经网络的难点</p>\\n<h4>VGG</h4>\\n<p>VGG指卷积神经网络模型，分别在2014 ImageNet定位和分类轨道中获得第一名和第二名。VGG模型由16-19个重量层构成，并使用尺寸为3×3和1×1的小型卷积滤波器。</p>\\n<p>用于大规模图像识别的非常深的卷积网络</p>\\n<h4>word2vec</h4>\\n<p>word2vec是一种通过尝试预测文档中单词的上下文来学习单词嵌入的算法和工具。例如，生成的单词向量具有一些有趣的属性。可以使用两个不同的目标来学习这些嵌入：Skip-Gram目标试图从一个单词预测上下文，并且CBOW目标试图从其上下文预测单词。vector('queen') ~= vector('king') - vector('man') + vector('woman')</p>\\n<p>向量空间中词表示的有效估计</p>\\n<p>词语的分布式表征及其组合性</p>\\n<p>word2vec参数学习解释</p>\"},\"indexJson\":{\"title\":\"深度学习词汇表\"}},\"pathContext\":{\"slug\":\"/posts/12/\",\"pid\":12,\"next\":null,\"prev\":\"/posts/11\"}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---posts-12-6245c00f4a64b0b54a4f.js","module.exports = {\"data\":{\"markdownRemark\":{\"html\":\"<h4>深度学习术语对于新手来说可能是相当的压倒性的。本词汇表试图定义常用术语，并链接到原始参考资料和其他资源，以帮助读者更深入地了解特定主题。</h4>\\n<h4>什么是深度学习与“一般”机器学习术语之间的界限是相当模糊的。我正在努力保留深度学习的术语表，但是这些决定有些武断。例如，我在这里不包括“交叉验证”，因为这是一种通用的技术，可以在机器学习中使用。不过，我已经决定加入softmax或word2vec等词语，因为它们通常与深度学习相关联，即使它们不是深度学习技术。</h4>\\n<h4>激活函数</h4>\\n<p>为了允许神经网络学习复杂的决策边界，我们对其某些层应用非线性激活函数。常用的功能包括sigmoid，tanh，ReLU（整流线性单位）及其变体。</p>\\n<h4>Adadelta</h4>\\n<p>Adadelta是基于梯度下降的学习算法，适应随时间变化的每个参数的学习速率。它被认为是对Adagrad的一种改进，Adagrad对超参数更为敏感，并可能过于积极地降低学习速度。Adadelta它与rmsprop类似，可以用来代替vanilla SGD。</p>\\n<h4>ADADELTA：一种自适应学习速率方法</h4>\\n<p>斯坦福CS231n：优化算法</p>\\n<p>梯度下降优化算法的概述</p>\\n<h4>Adagrad</h4>\\n<p>Adagrad是一种自适应学习速率算法，能够随时间跟踪平方梯度并自动调整每个参数的学习速率。可以使用它来代替vanilla SGD，对于稀疏数据尤其有用，它将更高的学习率分配给不经常更新的参数。</p>\\n<p>在线学习和随机优化的自适应次梯度方法</p>\\n<p>斯坦福CS231n：优化算法</p>\\n<p>梯度下降优化算法的概述</p>\\n<h4>亚当</h4>\\n<p>Adam是一种类似于rmsprop的自适应学习速率算法，但更新是\\n直接使用梯度的第一个和第二个时刻的移动平均值进行估计的，并且还包括偏差修正项。</p>\\n<p>亚当：随机优化的一种方法</p>\\n<p>梯度下降优化算法的概述</p>\\n<h4>Affine Layer</h4>\\n<p>神经网络中的完全连接层。仿射意味着前一层中的每个神经元都连接到当前层中的每个神经元。在很多方面，这是神经网络的“标准”层。在进行最终预测之前，仿射层通常被添加在卷积神经网络或递归神经网络的输出之上。仿射层通常形式的y = f(Wx + b)，其中x是该层的输入，W参数，b偏置向量，并f非线性激活函数</p>\\n<h4>Attention Mechanism</h4>\\n<p>注意机制受到人类视觉注意力的启发，能够专注于图像的特定部分。注意机制可以结合在语言处理和图像识别两种架构中，以帮助网络在进行预测时学习“关注”什么。</p>\\n<p>注意力和记忆深度学习和NLP</p>\\n<h4>Alexnet</h4>\\n<p>Alexnet是卷积神经网络体系结构的名称，它大大赢得了ILSVRC 2012大赛的冠军，并负责CNN在图像识别领域的兴趣。它由5个卷积层组成，其中一些是最大池层，另外三层是完全连接层，最后是1000个软最大值。Alexnet是在深度卷积神经网络的ImageNet分类中引入的。</p>\\n<h4>Autoencoder</h4>\\n<p>自动编码器是一个神经网络模型，其目标是预测输入本身，通常是通过网络某处的“瓶颈”。通过引入一个瓶颈，我们迫使网络学习输入的低维表示，有效地将输入压缩成一个良好的表示。自动编码器与PCA和其他降维技术相关，但由于它们的非线性本质，可以学习更复杂的映射。存在广泛的自编码器体系结构，包括去噪自动编码器，变化自动编码器或序列自动编码器。</p>\\n<h4>Average-Pooling</h4>\\n<p>平均汇集是用于图像识别的卷积神经网络中的汇集技术。它的工作原理是通过在像素之类的特征块上滑动窗口，并取得窗口内所有值的平均值。它将输入表示压缩为低维表示。</p>\\n<h4>Backpropagation</h4>\\n<p>反向传播是有效地计算神经网络中的梯度的算法，或更一般地说是前馈计算图。归结为从网络输出开始应用分化的链式规则，并向后传播梯度。反向传播的第一个用途可以追溯到20世纪60年代的Vapnik，但是通过反向传播错误的学习表示经常被引用为来源。</p>\\n<p>计算图演算：反向传播</p>\\n<h4>反向传播时间（BPTT）</h4>\\n<p>反向传播时间（纸）是应用于递归神经网络（RNN）的反向传播算法。BPTT可以被看作是应用于RNN的标准反向传播算法，其中每个时间步代表一个层，参数在层间共享。由于RNN在所有时间步骤中共享相同的参数，因此一次性步骤中的错误必须“全部”时间地反向传播到所有以前的时间步骤，因此也就是名称。处理长序列（数百个输入）时，经常使用截断版本的BPTT来降低计算成本。截断的BPTT会在固定数量的步骤后停止反向传播错误。</p>\\n<p>反向传播的时间：它做什么和如何做到这一点</p>\\n<h4>Batch Normalization</h4>\\n<p>批量标准化是一种技术，使每个最小批量的层输入标准化。它加快培训，允许使用更高的学习率，并可以充当正规化者。批量归一化对于卷积和前馈神经网络是非常有效的，但还没有被成功应用于递归神经网络。</p>\\n<p>批量标准化：通过减少内部协变量加速深度网络培训</p>\\n<p>批量归一化递归神经网络</p>\\n<h4>Bidirectional RNN</h4>\\n<p>双向递归神经网络是一种包含两个不同方向的RNN的神经网络。前向RNN从开始到结束读取输入序列，而后向RNN从结束到开始读取输入序列。两个RNN堆叠在一起，通常通过附加两个向量来组合它们的状态。双向RNN经常被用在自然语言问题中，在进行预测之前，我们要考虑单词前后的上下文。</p>\\n<p>双向递归神经网络</p>\\n<h4>Caffe</h4>\\n<p>Caffe是由伯克利视觉和学习中心开发的深度学习框架。Caffe对于视觉任务和CNN模型特别受欢迎和表现。</p>\\n<h4>Categorical Cross-Entropy Loss</h4>\\n<p>分类交叉熵损失也被称为负对数似然值。这是一个流行的分类问题的损失函数，并测量两个概率分布之间的相似性，通常是真实的标签和预测的标签。它由下式给出L = -sum(y * log(y<em>prediction))其中y为真标签的概率分布（通常是一个热载体）和y</em>prediction是预测标签的概率分布，通常从一个未来SOFTMAX。</p>\\n<h4>Channel</h4>\\n<p>向深度学习模型输入数据可以有多个通道。规范示例是具有红色，绿色和蓝色通道的图像。图像可以被表示为具有与通道，高度和宽度相对应的尺寸的三维张量。自然语言数据也可以具有多个通道，例如以不同类型的嵌入的形式。</p>\\n<h4>Convolutional Neural Network (CNN, ConvNet)</h4>\\n<p>CNN使用卷积来连接输入的局部区域的提取特征。大多数CNN包含卷积，合并和仿射层的组合。有线电视网络已经获得了普及，尤其是通过在视觉识别任务方面的卓越表现，他们已经在几年时间里设定了艺术水平。</p>\\n<p>斯坦福CS231n类 - 用于视觉识别的卷积神经网络</p>\\n<p>了解用于NLP的卷积神经网络</p>\\n<h4>Deep Belief Network (DBN)</h4>\\n<p>DBN是一种概率图形模型，以无监督的方式学习数据的分层表示。DBN由多个隐藏层组成，每个连续层对中的神经元之间都有连接。DBN是通过将多个RBN堆叠在一起并逐一进行培训而建立的。</p>\\n<p>一种深度信仰网络的快速学习算法</p>\\n<h4>Deep Dream</h4>\\n<p>谷歌发明的一种技术，试图提炼深层卷积神经网络所捕获的知识。该技术可以生成新的图像，或者转换现有的图像，并赋予它们梦幻般的味道，尤其是当递归地应用时。</p>\\n<p>在Github上的深刻的梦想</p>\\n<p>启示主义：深入到神经网络</p>\\n<h4>Dropout</h4>\\n<p>辍学是防止过度拟合的神经网络正则化技术。它通过在每次训练迭代中随机地将它们的一部分设置为0来防止神经元进行协调。丢失可以用各种方式来解释，例如从指数的不同网络随机抽样。Dropout层首先通过在CNN中的使用而获得了普及，但是之后已经被应用于其他层，包括输入嵌入或循环网络。</p>\\n<p>辍学：防止神经网络过度配置的一种简单方法</p>\\n<p>递归神经网络正则化</p>\\n<h4>Embedding</h4>\\n<p>嵌入将输入表示（如单词或句子）映射到向量中。一个流行的嵌入类型是词嵌入，如word2vec或GloVe。我们也可以嵌入句子，段落或图像。例如，通过将图像及其文字描述映射到一个共同的嵌入空间，并将它们之间的距离最小化，我们可以将标签与图像进行匹配。可以明确地学习嵌入，例如在word2vec中，或作为受监督任务的一部分，如情感分析。通常，网络的输入层用预先训练好的嵌入进行初始化，然后对其进行微调以适应当前的任务。</p>\\n<h4>Exploding Gradient Problem</h4>\\n<p>爆炸梯度问题与消失梯度问题相反。在深度神经网络中，反向传播期间梯度可能会发生爆炸，导致数字溢出。处理爆炸渐变的常用技术是执行渐变裁剪。</p>\\n<p>训练递归神经网络的难点</p>\\n<h4>Fine-Tuning</h4>\\n<p>微调是指利用来自其他任务的参数（例如无监督的训练任务）来初始化网络，然后基于当前任务更新这些参数的技术。例如，NLP体系结构通常使用像word2vec这样的预先训练的词嵌入，然后根据特定任务（如情感分析）在训练期间更新这些词嵌入。</p>\\n<h4>Gradient Clipping</h4>\\n<p>梯度剪切是一种防止在深度网络（通常是递归神经网络）中发生梯度变化的技术。梯度裁剪存在多种方式，但常用的方法是当L2范数超过一定阈值时，对参数矢量的梯度进行归一化new<em>gradients = gradients * threshold / l2</em>norm(gradients)。</p>\\n<p>训练递归神经网络的难点</p>\\n<h4>GloVe</h4>\\n<p>GloVe是一种无监督学习算法，用于获取单词的矢量表示（嵌入）。Glove矢量与word2vec具有相同的用途，但由于在同现统计方面进行了训练，因此具有不同的矢量表示。</p>\\n<p>GloVe：词表示的全局向量</p>\\n<h4>GoogleLeNet</h4>\\n<p>赢得2014年ILSVRC挑战的卷积神经网络架构的名称。网络使用Inception模块来减少参数，提高网络内部计算资源的利用率。</p>\\n<p>越来越深入的卷积</p>\\n<h4>GRU</h4>\\n<p>门控重复单元是一个参数较少的LSTM单元的简化版本。就像LSTM单元一样，它使用门控机制来允许RNN通过防止消失梯度问题来有效地学习远程依赖。GRU由一个复位和更新门组成，用于确定旧存储器的哪一部分保持与当前时间步的新值更新。</p>\\n<p>使用RNN编码器 - 解码器来学习用于统计机器翻译的短语表示</p>\\n<p>复发神经网络教程，第4部分 - 用Python和Theano实现GRU / LSTM RNN</p>\\n<h4>Highway Layer</h4>\\n<p>高速公路层（纸）是一种神经网络层，它使用门控机制来控制通过层的信息流。堆叠多个公路层允许训练非常深的网络。高速公路层通过学习一个选通功能来工作，该选通功能选择输入的哪些部分通过，哪些部分通过变换功能，例如标准的仿射层。高速公路层的基本表述是T * h(x) + (1 - T) * x，其中T值为0和1之间的学习门控函数h(x)是任意的输入变换并且x是输入。请注意，所有这些都必须具有相同的大小。</p>\\n<h4>ICML</h4>\\n<p>该国际会议的机器学习，一个顶级的机器学习会议。</p>\\n<h4>ILSVRC</h4>\\n<p>所述ImageNet大型视觉辨识挑战在大规模的计算结果为对象检测和图像分类算法。这是计算机视觉领域最受欢迎的学术挑战。在过去的几年里，深度学习技术使错误率从30％显着降低到不到5％，在几个分类任务中击败了人的表现。</p>\\n<h4>Inception Module</h4>\\n<p>启发模块被用于卷积神经网络，以允许更有效的计算和更深的网络通过叠加1×1卷积的维度降低。</p>\\n<p>越来越深入的卷积</p>\\n<h4>Keras</h4>\\n<p>Kears是一个基于Python的深度学习库，包含许多用于深度神经网络的高级构建模块。它可以在TensorFlow，Theano或CNTK之上运行。</p>\\n<h4>LSTM</h4>\\n<p>通过使用记忆门控机制，发明了长期短期记忆网络来防止递归神经网络中的消失梯度问题。使用LSTM单元计算RNN中的隐藏状态，我们帮助网络高效地传播梯度和学习远程依赖性。</p>\\n<p>长期的短期记忆</p>\\n<p>了解LSTM网络</p>\\n<p>复发神经网络教程，第4部分 - 用Python和Theano实现GRU / LSTM RNN</p>\\n<h4>Max-Pooling</h4>\\n<p>一池操作通常在卷积神经网络中使用。最大池图层从一个特征块中选择最大值。就像卷积图层一样，通过窗口（补丁）大小和跨度大小来对池化层进行参数化。例如，我们可以使用步长大小2在10×10特征矩阵上滑动大小为2×2的窗口，在每个窗口内的所有4个值中选择最大值，从而产生新的5×5特征矩阵。合并层通过仅保留最显着的信息来帮助减少表示的维度，并且在图像输入的情况下，它们为翻译提供基本的不变性（即使图像被移动几个像素，也将选择相同的最大值）。汇聚层通常插入连续的卷积层之间。</p>\\n<h4>MNIST</h4>\\n<p>该MNIST数据集是也许是最常用的图像识别数据集。它包括6万个训练和10,000个手写数字的测试例子。每个图像是28×28像素大。现有技术模型通常在测试集上达到99.5％或更高的精确度。</p>\\n<h4>Momentum</h4>\\n<p>动量是梯度下降算法的扩展，可以加速或抑制参数更新。在实践中，包括渐变下降更新中的动量项导致深度网络中更好的收敛速率。</p>\\n<p>通过向后传播错误学习表示</p>\\n<h4>Multilayer Perceptron (MLP(</h4>\\n<p>多层感知器是具有多个完全连接层的前馈神经网络，其使用非线性激活函数来处理不能线性分离的数据。MLP是多层神经网络的最基本的形式，或者如果它具有超过两层的话，则是深度神经网络。</p>\\n<h4>Negative Log Likelihood (NLL)</h4>\\n<p>请参阅分类交叉熵损失。</p>\\n<h4>Neural Machine Translation (NMT)</h4>\\n<p>NMT系统使用神经网络在诸如英语和法语等语言之间进行翻译。可以使用双语语料库对NMT系统进行端到端的培训，这与需要手工特征和工程设计的传统机器翻译系统不同。NMT系统通常使用编码器和解码器递归神经网络来实现，其分别对源句子进行编码并产生目标句子。</p>\\n<p>用神经网络进行序列学习的顺序</p>\\n<p>使用RNN编码器 - 解码器来学习用于统计机器翻译的短语表示</p>\\n<h4>Neural Turing Machine (NTM)</h4>\\n<p>NMT是可以从例子推断简单算法的神经网络架构。例如，NTM可以通过示例输入和输出来学习排序算法。NTM通常学习某种形式的记忆和注意机制来处理程序执行期间的状态。</p>\\n<p>神经图灵机</p>\\n<h4>非线性Nonlinearity</h4>\\n<p>请参阅激活功能。</p>\\n<h4>Noise-contrastive estimation (NCE)</h4>\\n<p>噪声对比估计是通常用于训练具有大量输出词汇的分类器的抽样损失。计算大量可能的类的softmax是非常昂贵的。使用NCE，我们可以通过训练分类器来区分样本与“真实”分布和人工生成的噪声分布，从而将问题降低到二元分类问题。</p>\\n<p>噪声对比估计：非标准统计模型的一种新的估计原理</p>\\n<p>利用噪声对比估计有效地学习单词嵌入</p>\\n<h4>Pooling</h4>\\n<p>请参阅最大池或平均池。</p>\\n<h4>Restricted Boltzmann Machine (RBN)</h4>\\n<p>RBM是一种概率图模型，可以被解释为一个随机的人工神经网络。RBN以无监督的方式学习数据的表示。RBN由可见和隐藏层以及这些层中每个层中的二元神经元之间的连接组成。RBN可以使用对比发散（梯度下降的近似）进行有效训练。</p>\\n<p>第六章：动力系统中的信息处理：和谐理论的基础</p>\\n<p>限制玻尔兹曼机介绍</p>\\n<h4>递归神经网络（RNN）</h4>\\n<p>RNN通过隐藏状态或存储器模拟顺序交互。它可以占用N个输入并产生多达N个输出。例如，输入序列可以是输出是每个词（N到N）的词性标签的句子。输入可以是一个句子，并且输出一个句子的情感分类（N-to-1）。输入可以是单个图像，并且输出可以是对应于图像描述（1到N）的一系列单词。在每个时间步，RNN基于当前输入和先前的隐藏状态计算新的隐藏状态（“记忆”）。“经常性”来源于这样一个事实，即在每个步骤中使用相同的参数，网络根据不同的输入执行相同的计算。</p>\\n<p>了解LSTM网络</p>\\n<p>复发神经网络教程，第1部分 - RNN简介</p>\\n<p>递归神经网络\\n递归神经网络的泛化回归神经网络的树状结构。在每个递归中应用相同的权重。就像RNN一样，递归神经网络可以使用反向传播进行端对端训练。尽管可以将树结构学习为优化问题的一部分，但是递归神经网络通常应用于已经具有预定义结构的问题，如自然语言处理中的分析树。</p>\\n<p>用递归神经网络解析自然场景和自然语言</p>\\n<h4>RELU</h4>\\n<p>整流线性单位的缩写。在深度神经网络中，ReLUs经常用作激活功能。他们被定义f(x) = max(0, x)。ReLU相对于函数的优点tanh包括它们往往是稀疏的（它们的激活容易被设置为0），并且它们受渐变问题的影响较小。ReLUs是卷积神经网络中最常用的激活函数。存在多种ReLU的变体，例如泄漏ReLU，参数ReLU（PReLU）或更平滑的Softplus近似。</p>\\n<p>深入整流器：在ImageNet分类上超越人类级别的性能</p>\\n<p>整流器非线性改善神经网络声学模型</p>\\n<p>整流线性单元改进受限玻尔兹曼机器</p>\\n<h4>RESNET</h4>\\n<p>Deep Residual Networks赢得了2015年ILSVRC挑战赛。这些网络通过在堆栈层之间引入快捷连接起作用，允许优化器学习“更容易”的残差映射，而不是更复杂的原始映射。这些快捷连接与高速公路层相似，但它们与数据无关，不会引入其他参数或培训复杂性。ResNet在ImageNet测试集中达到了3.57％的错误率。</p>\\n<p>用于图像识别的深度残差学习</p>\\n<h4>RMSProp</h4>\\n<p>RMSProp是一个基于梯度的优化算法。它与Adagrad类似，但引入了一个额外的衰减术语来抵消Adagrad的学习速度的迅速下降。</p>\\n<p>机器学习的神经网络讲座6a</p>\\n<p>斯坦福CS231n：优化算法</p>\\n<p>梯度下降优化算法的概述</p>\\n<h4>Seq2Seq</h4>\\n<p>序列 - 序列模型读取序列（如句子）作为输入，并生成另一个序列作为输出。它与标准的RNN不同之处在于，在网络开始产生任何输出之前，输入序列被完全读取。通常情况下，seq2seq模型使用两个RNN来实现，作为编码器和解码器。神经机器翻译是一个seq2seq模型的典型例子。</p>\\n<p>用神经网络进行序列学习的顺序</p>\\n<h4>SGD</h4>\\n<p>随机梯度下降（Wikipedia）是一种基于梯度的优化算法，用于在训练阶段学习网络参数。通常使用反向传播算法计算梯度。在实践中，人们使用SGD的minibatch版本，其中参数更新是基于批处理而不是单个示例执行的，从而提高了计算效率。存在许多对香草SGD的扩展，包括Momentum，Adagrad，rmsprop，Adadelta或Adam。</p>\\n<p>在线学习和随机优化的自适应次梯度方法</p>\\n<p>斯坦福CS231n：优化算法</p>\\n<p>梯度下降优化算法的概述</p>\\n<h4>SOFTMAX</h4>\\n<p>该SOFTMAX功能通常用于原始分数的向量转换成类概率在一个神经网络用于分类的输出层。它通过指数归一化和归一化常数除以归一化得分。如果我们正在处理大量的类，例如机器翻译中的大量词汇，那么归一化常数计算起来就很昂贵。存在使计算更有效的各种备选方案，包括分层Softmax或使用诸如NCE的基于抽样的损失。</p>\\n<h4>TensorFlow</h4>\\n<p>TensorFlow是一个开源的C ++ / Python软件库，用于使用数据流图进行数值计算，特别是深度神经网络。它是由Google创建的。在设计上，它与Theano最为相似，比Caffe或Keras更低。</p>\\n<h4>Theano</h4>\\n<p>Theano是一个Python库，允许您定义，优化和评估数学表达式。它包含许多用于深度神经网络的构件。Theano是一个类似于Tensorflow的低级库。更高级的图书馆包括Keras和Caffe。</p>\\n<h4>Vanishing Gradient Problem</h4>\\n<p>消失梯度问题出现在深度非常深的神经网络中，通常为递归神经网络，它使用梯度趋于较小（范围从0到1）的激活函数。由于这些小梯度在反向传播过程中倍增，因此它们倾向于在整个层中“消失”，从而阻止网络学习远程依赖性。解决这个问题的常用方法是使用像ReLU这样的激活函数，这些函数不会遇到小的梯度，或者使用像LSTM这样的体系结构来明确消除渐变梯度。这个问题的反面被称为爆炸的梯度问题。</p>\\n<p>训练递归神经网络的难点</p>\\n<h4>VGG</h4>\\n<p>VGG指卷积神经网络模型，分别在2014 ImageNet定位和分类轨道中获得第一名和第二名。VGG模型由16-19个重量层构成，并使用尺寸为3×3和1×1的小型卷积滤波器。</p>\\n<p>用于大规模图像识别的非常深的卷积网络</p>\\n<h4>word2vec</h4>\\n<p>word2vec是一种通过尝试预测文档中单词的上下文来学习单词嵌入的算法和工具。例如，生成的单词向量具有一些有趣的属性。可以使用两个不同的目标来学习这些嵌入：Skip-Gram目标试图从一个单词预测上下文，并且CBOW目标试图从其上下文预测单词。vector('queen') ~= vector('king') - vector('man') + vector('woman')</p>\\n<p>向量空间中词表示的有效估计</p>\\n<p>词语的分布式表征及其组合性</p>\\n<p>word2vec参数学习解释</p>\"},\"indexJson\":{\"title\":\"深度学习词汇表\"}},\"pathContext\":{\"slug\":\"/posts/12/\",\"pid\":12,\"next\":null,\"prev\":\"/posts/11\"}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/posts-12.json\n// module id = 361\n// module chunks = 279941687692024"],"sourceRoot":""}