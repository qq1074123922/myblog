webpackJsonp([0xde32420b160b],{360:function(n,e){n.exports={data:{markdownRemark:{html:'<p>在之前的教程中，我们介绍了 Keras 网络的模型与网络层，并且通过许多示例展示了网络的搭建方式。大家都注意到了，在构建网络的过程中，损失函数、优化器、激活函数等都是需要自定义的网络配置项，下面我们对这些网络配置进行详细的介绍。</p>\n<ol>\n<li>\n<p>损失函数\n目标函数 objectives\n目标函数，或称损失函数，是编译一个模型必须的两个参数之一：</p>\n<div class="gatsby-highlight">\n      <pre class="language-python"><code>model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">\'mean_squared_error\'</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span><span class="token string">\'sgd\'</span><span class="token punctuation">)</span>\n可以通过传递预定义目标函数名字指定目标函数，也可以传递一个 Theano<span class="token operator">/</span>TensroFlow 的符号函数作为目标函数，该函数对每个数据点应该只返回一个标量值，并以下列两个参数为参数：\n</code></pre>\n      </div>\n</li>\n</ol>\n<p>y<em>true：真实的数据标签，Theano/TensorFlow 张量\ny</em>pred：预测值，与 y_true 相同 shape 的 Theano/TensorFlow 张量\nfrom keras import losses</p>\n<p>model.compile(loss=losses.mean<em>squared</em>error, optimizer=\'sgd\')\n真实的优化目标函数是在各个数据点得到的损失函数值之和的均值。</p>\n<p>预定义目标函数\nmean<em>squared</em>error 或 mse：均方误差\nmean<em>absolute</em>error 或 mae：平均绝对误差\nmean<em>absolute</em>percentage<em>error 或 mape：平均绝对百分比误差\nmean</em>squared<em>logarithmic</em>error 或 msle：均方误差对数\nsquared<em>hinge\nhinge\nbinary</em>crossentropy：对数损失，logloss\nlogcosh\ncategorical<em>crossentropy：亦称作多类的对数损失，注意使用该目标函数时，需要将标签转化为形如 (nb</em>samples, nb<em>classes) 的二值序列\nsparse</em>categorical<em>crossentrop：如上，但接受稀疏标签。注意，使用该函数时仍然需要你的标签与输出值的维度相同，你可能需要在标签数据上增加一个维度：np.expand</em>dims(y,-1)\nkullback<em>leibler</em>divergence：从预测值概率分布 Q 到真值概率分布 P 的信息增益，用以度量两个分布的差异\npoisson：即 (predictions - targets * log(predictions)) 的均值\ncosine<em>proximity：即预测值与真实标签的余弦距离平均值的相反数\n注意: 当使用”categorical</em>crossentropy”作为目标函数时，标签应该为多类模式，即 one-hot 编码的向量，而不是单个数值.。可以使用工具中的 to_categorical 函数完成该转换，示例如下：</p>\n<p>from keras.utils.np<em>utils import to</em>categorical</p>\n<p>categorical<em>labels = to</em>categorical(int<em>labels, num</em>classes=None)</p>\n<div class="gatsby-highlight">\n      <pre class="language-none"><code>2. 优化器\n优化器是编译 Keras 模型必要的两个参数之一。\n```python\nfrom keras import optimizers\n\nmodel = Sequential()\nmodel.add(Dense(64, init=\'uniform\', input_shape=(10,)))\nmodel.add(Activation(\'tanh\'))\nmodel.add(Activation(\'softmax\'))\n\nsgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss=\'mean_squared_error\', optimizer=sgd)\n可以在调用 model.compile() 之前初始化一个优化器对象，然后传入该函数（如上所示），也可以在调用 model.compile() 时传递一个预定义优化器名。在后者情形下，优化器的参数将使用默认值。\n\n# 传递一个预定义优化器名\nmodel.compile(loss=\'mean_squared_error\', optimizer=\'sgd\')\n所有优化器都可用的参数\n参数 clipnorm 和 clipvalue 是所有优化器都可以使用的参数，用于对梯度进行裁剪。示例如下：\n\nfrom keras import optimizers\n\n# 所有参数的梯度会被剪裁到最大范数为1.\nsgd = optimizers.SGD(lr=0.01, clipnorm=1.)\nfrom keras import optimizers\n\n# 所有参数梯度会被剪裁到最大值为0.5，最小值为-0.5\nsgd = optimizers.SGD(lr=0.01, clipvalue=0.5)\nSGD\n随机梯度下降法，支持动量参数，支持学习衰减率，支持 Nesterov 动量。\n\nkeras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\nlr：大于 0 的浮点数，学习率\nmomentum：大于 0 的浮点数，动量参数\ndecay：大于 0 的浮点数，每次更新后的学习率衰减值\nnesterov：布尔值，确定是否使用 Nesterov 动量\nRMSprop\n除学习率可调整外，建议保持优化器的其他默认参数不变。该优化器通常是面对递归神经网络时的一个良好选择。\n\nkeras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)\nlr：大于 0 的浮点数，学习率\nrho：大于 0 的浮点数\nepsilon：大于 0 的小浮点数，防止除 0 错误\nAdam\n该优化器的默认值来源于参考文献。\n\nkeras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\nlr：大于 0 的浮点数，学习率\nbeta_1：浮点数， 0<beta<1，通常很接近 1\nbeta_2：浮点数， 0<beta<1，通常很接近 1\nepsilon：大于 0的小浮点数，防止除 0 错误</code></pre>\n      </div>\n<ol start="3">\n<li>\n<p>激活函数\n激活函数用于在模型中引入非线性。激活函数 </p>\n<div class="gatsby-highlight">\n      <pre class="language-python"><code>sigmoid\nsigmoid​ 与 \ntanh\ntanh​ 曾经很流行，但现在很少用于视觉模型了，主要原因在于当输入的绝对值较大时，其导数接近于零，梯度的反向传播过程将被中断，出现梯度消散的现象。\n</code></pre>\n      </div>\n</li>\n</ol>\n<p>1</p>\n<p>ReLU\nReLU 是一个很好的替代：</p>\n<p>2</p>\n<p>相比于\nsigmoid\nsigmoid 与\ntanh\ntanh，它有两个优势：</p>\n<p>没有饱和问题，大大缓解了梯度消散的现象，加快了收敛速度。\n实现起来非常简单，加速了计算过程。\nReLU\nReLU 有一个缺陷，就是它可能会永远“死”掉：</p>\n<p>假如有一组二维数据\nX\nX\n(\nx\n1\n,\nx\n2\n)\n(x1,x2) 分布在\nx\n1\n:\n[\n0\n,\n1\n]\n,\nx\n2\n:\n[\n0\n,\n1\n]\nx1:[0,1],x2:[0,1] 的区域内，有一组参数\nW\nW\n(\nw\n1\n,\nw\n2\n)\n(w1,w2) 对\nX\nX 做线性变换，并将结果输入到\nReLU\nReLU。\nF\n=\nw\n1\n∗\nx\n1\n+\nw\n2\n∗\nx\n2\nF=w1∗x1+w2∗x2 如果\nw\n1\n=\nw\n2\n=\n−\n1\nw1=w2=−1，那么无论\nX\nX 如何取值，\nF\nF 必然小于等于零。那么\nReLU\nReLU 函数对\nF\nF 的导数将永远为零。这个\nReLU\nReLU 节点将永远不参与整个模型的学习过程。</p>\n<p>造成上述现象的原因是\nReLU\nReLU 在负区间的导数为零，为了解决这一问题，人们发明了\nLeaky ReLU\nLeaky ReLU、\nParametric ReLU\nParametric ReLU、\nRandomized ReLU\nRandomized ReLU 等变体。他们的中心思想都是为\nReLU\nReLU 函数在负区间赋予一定的斜率，从而让其导数不为零（这里设斜率为\nα\nα）。</p>\n<p>Leaky ReLU\nLeaky ReLU 就是直接给\nα\nα 指定一个值，整个模型都用这个斜率：</p>\n<p>3</p>\n<p>Parametric ReLU\nParametric ReLU 将\nα\nα 作为一个参数，通过学习获取它的最优值。\nRandomized ReLU\nRandomized ReLU 为\nα\nα 规定一个区间，然后在区间内随机选取\nα\nα 的值。</p>\n<p>4</p>\n<p>在实践中，\nParametric ReLU\nParametric ReLU 和\nRandomized ReLU\nRandomized ReLU 都是可取的。</p>\n<p>激活函数可以通过设置单独的激活层实现，也可以在构造层对象时通过传递 activation 参数实现。</p>\n<p>from keras.layers import Activation, Dense</p>\n<p>model.add(Dense(64))\nmodel.add(Activation(\'tanh\'))\n等价于：</p>\n<p>model.add(Dense(64, activation=\'tanh\'))\n也可以通过传递一个逐元素运算的 Theano/TensorFlow 函数来作为激活函数：</p>\n<p>from keras import backend as K</p>\n<p>def tanh(x):\nreturn K.tanh(x)</p>\n<p>model.add(Dense(64, activation=tanh))\nmodel.add(Activation(tanh)\n预定义激活函数\nsoftmax：对输入数据的最后一维进行 softmax，输入数据应形如 (nb<em>samples, nb</em>timesteps, nb<em>dims) 或 (nb</em>samples, nb<em>dims)\nelu\nsoftplus\nsoftsign\nrelu\ntanh\nsigmoid\nhard</em>sigmoid\nlinear\n高级激活函数\n对于简单的 Theano/TensorFlow 不能表达的复杂激活函数，如含有可学习参数的激活函数，可通过高级激活函数实现，如 PReLU，LeakyReLU 等。下面我们详细介绍一下高级激活层。</p>\n<div class="gatsby-highlight">\n      <pre class="language-none"><code>4. 高级激活层\nLeakyReLU 层\nLeakyRelU 是修正线性单元(Rectified Linear Unit，ReLU)的特殊版本，当不激活时，LeakyReLU 仍然会有非零输出值，从而获得一个小梯度，避免 ReLU 可能出现的神经元“死亡”现象。\n```python\nkeras.layers.advanced_activations.LeakyReLU(alpha=0.3)\nalpha：大于 0 的浮点数，代表激活函数图像中第三象限线段的斜率\n该层输入 shape 任意，当使用该层为模型首层时需指定 input_shape 参数，输出 shape 与输入相同。\n\nPReLU 层\n该层为参数化的 ReLU(Parametric ReLU)，表达式是：\nf\n(\nx\n)\n=\n{\nα\n∗\nx\n,\nx\n<\n0\nx\n,\nx\n≥\n0\nf(x)={α∗x,x<0x,x≥0，此处的 \nα\nα 为一个与 xshape 相同的可学习的参数向量。\n\nkeras.layers.advanced_activations.PReLU(shared_axes=None)\nshared_axes：该参数指定的轴将共享同一组科学系参数，例如假如输入特征图是从 2D 卷积过来的，具有形如 (batch, height, width, channels) 这样的 shape，则或许你会希望在空域共享参数，这样每个 filter 就只有一组参数，设定 shared_axes=[1,2] 可完成该目标\nELU 层\nELU 层是指数线性单元(Exponential Linera Unit)，表达式为： 该层为参数化的 ReLU(Parametric ReLU)，表达式是：\nf\n(\nx\n)\n=\n{\nα\n∗\n(\nexp\n(\nx\n)\n−\n1\n)\n,\nx\n<\n0\nx\n,\nx\n≥\n0\nf(x)={α∗(exp⁡(x)−1),x<0x,x≥0。\n\nkeras.layers.advanced_activations.ELU(alpha=1.0)\nalpha：控制负因子的参数\n该层输入 shape 任意，当使用该层为模型首层时需指定 input_shape 参数，输出 shape 与输入相同。\n\nThresholdedReLU 层\n该层是带有门限的 ReLU，表达式是：\nf\n(\nx\n)\n=\n{\nx\n,\nx\n>\nθ\n0\n,\notherwise\nf(x)={x,x>θ0,otherwise。\n\nkeras.layers.advanced_activations.ThresholdedReLU(theta=1.0)\ntheata：大或等于 0 的浮点数，激活门限位置\n该层输入 shape 任意，当使用该层为模型首层时需指定 input_shape 参数，输出 shape 与输入相同。</code></pre>\n      </div>\n<ol start="5">\n<li>\n<p>数据预处理\n填充序列\n将长为 nb<em>samples 的序列（标量序列）转化为形如 (nb</em>samples,nb<em>timesteps) 的 2D numpy array。如果提供了参数 maxlen，nb</em>timesteps=maxlen，否则其值为最长序列的长度。其他短于该长度的序列都会在后部填充 0以达到该长度。长于 nb_timesteps 的序列将会被截断，以使其匹配目标长度。padding 和截断发生的位置分别取决于 padding 和 truncating。</p>\n<div class="gatsby-highlight">\n      <pre class="language-python"><code>keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>sequence<span class="token punctuation">.</span>pad_sequences<span class="token punctuation">(</span>sequences<span class="token punctuation">,</span> maxlen<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">\'int32\'</span><span class="token punctuation">,</span>\npadding<span class="token operator">=</span><span class="token string">\'pre\'</span><span class="token punctuation">,</span> truncating<span class="token operator">=</span><span class="token string">\'pre\'</span><span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">)</span>\nsequences：浮点数或整数构成的两层嵌套列表\nmaxlen：<span class="token boolean">None</span> 或整数，为序列的最大长度。大于此长度的序列将被截短，小于此长度的序列将在后部填 <span class="token number">0</span>\ndtype：返回的 numpy array 的数据类型\npadding：“pre”或“post”，确定当需要补 <span class="token number">0</span> 时，在序列的起始还是结尾补\ntruncating：“pre”或“post”，确定当需要截断序列时，从起始还是结尾截断\nvalue：浮点数，此值将在填充时代替默认的填充值 <span class="token number">0</span>\n返回值返回形如 <span class="token punctuation">(</span>nb_samples<span class="token punctuation">,</span>nb_timesteps<span class="token punctuation">)</span> 的 <span class="token number">2D</span> 张量。\n</code></pre>\n      </div>\n</li>\n</ol>\n<p>句子分割\n本函数将一个句子拆分成单词构成的列表。</p>\n<p>keras.preprocessing.text.text<em>to</em>word<em>sequence(text, filters=base</em>filter(), lower=True, split=" ")\ntext：字符串，待处理的文本\nfilters：需要滤除的字符的列表或连接形成的字符串，例如标点符号。默认值为base_filter()，包含标点符号，制表符和换行符等\nlower：布尔值，是否将序列设为小写形式\nsplit：字符串，单词的分隔符，如空格\n返回值为字符串列表。</p>\n<p>one-hot 编码\n本函数将一段文本编码为 one-hot 形式的码，即仅记录词在词典中的下标。</p>\n<p>从定义上，当字典长为 n 时，每个单词应形成一个长为 n 的向量，其中仅有单词本身在字典中下标的位置为 1，其余均为 0，这称为 one-hot。</p>\n<p>为了方便起见，函数在这里仅把“1”的位置，即字典中词的下标记录下来。</p>\n<p>keras.preprocessing.text.one<em>hot(text, n, filters=base</em>filter(), lower=True, split=" ")\ntext：字符串，待处理的文本\nn：整数，字典长度\nfilters：需要滤除的字符的列表或连接形成的字符串，例如标点符号。默认值为base_filter()，包含标点符号，制表符和换行符等\nlower：布尔值，是否将序列设为小写形式\nsplit：字符串，单词的分隔符，如空格\n返回值为整数列表，每个整数是\n<a href="">\n1\n,\nn\n</a> 之间的值，代表一个单词（不保证唯一性，即如果词典长度不够，不同的单词可能会被编为同一个码）。</p>\n<p>分词器\nTokenizer 是一个用于向量化文本，或将文本转换为序列（即单词在字典中的下标构成的列表，从 1 算起）的类。</p>\n<p>keras.preprocessing.text.Tokenizer(nb<em>words=None, filters=base</em>filter(), lower=True, split=" ")\nnb<em>words：None 或整数，处理的最大单词数量。若被设置为整数，则分词器将被限制为处理数据集中最常见的 nb</em>words-1 个单词\nfilters：需要滤除的字符的列表或连接形成的字符串，例如标点符号。默认值为base_filter()，包含标点符号，制表符和换行符等\nlower：布尔值，是否将序列设为小写形式\nsplit：字符串，单词的分隔符，如空格\n类方法：</p>\n<p>fit<em>on</em>texts(texts)\ntexts：要用以训练的文本列表\ntexts<em>to</em>sequences(texts)\ntexts：待转为序列的文本列表\n返回值：序列的列表，列表中每个序列对应于一段输入文本\ntexts<em>to</em>sequences<em>generator(texts)\n本函数是 texts</em>to<em>sequences 的生成器函数版\ntexts：待转为序列的文本列表\n返回值：每次调用返回对应于一段输入文本的序列\ntexts</em>to<em>matrix(texts, mode)：\ntexts：待向量化的文本列表\nmode：“binary”、“count”、“tfidf”、“freq”之一，默认为“binary”\n返回值：形如 (len(texts), nb</em>words) 的 numpy array\nfit<em>on</em>sequences(sequences):\nsequences：要用以训练的序列列表\nsequences<em>to</em>matrix(sequences):\nsequences：待向量化的序列列表\nmode：“binary”、“count”、“tfidf”、“freq”之一，默认为“binary”\n返回值：形如 (len(sequences), nb_words) 的 numpy array\n属性：</p>\n<p>word<em>counts：字典，将单词（字符串）映射为它们在训练期间出现的次数。仅在调用 fit</em>on<em>texts 之后设置。\nword</em>docs：字典，将单词（字符串）映射为它们在训练期间所出现的文档或文本的数量。仅在调用 fit<em>on</em>texts 之后设置。\nword<em>index：字典，将单词（字符串）映射为它们的排名或者索引。仅在调用 fit</em>on<em>texts 之后设置。\ndocument</em>count：整数。分词器被训练的文档（文本或者序列）数量。仅在调用 fit<em>on</em>texts 或 fit<em>on</em>sequences 之后设置。</p>\n<div class="gatsby-highlight">\n      <pre class="language-none"><code></code></pre>\n      </div>'},indexJson:{title:"Keras 快速上手指南（下）"}},pathContext:{slug:"/posts/11/",pid:11,next:"/posts/12",prev:"/posts/10"}}}});
//# sourceMappingURL=path---posts-11-64142ec9824c2a7d7728.js.map